# ç¬¬ä¸€ç¯‡è®ºæ–‡å®Œæ•´å¤§çº²ä¸æŠ€æœ¯æ–¹æ¡ˆ

**æ ‡é¢˜**: Adaptive Pyramid-Based Cross-Modal Knowledge Distillation for Vision-Language Models

**è‹±æ–‡ç¼©å†™**: CMAPKD

**ç›®æ ‡ä¼šè®®**: ECCV 2026 (æˆªç¨¿: 2026å¹´3æœˆ)

---

## ğŸ“‹ è®ºæ–‡å®Œæ•´å¤§çº² (8é¡µECCVæ ¼å¼)

### 1. Abstract (200-250è¯)

**ç»“æ„**:
```
[é—®é¢˜] Vision-language models (VLMs) achieve impressive performance but
       suffer from high computational costs, hindering deployment.

[ç°æœ‰æ–¹æ³•å±€é™] While knowledge distillation offers model compression, existing
              methods either use pyramid structures for inference enhancement
              (LLaVA-UHD) or perform distillation without leveraging multi-scale
              features (LLaVA-KD).

[æœ¬æ–‡æ–¹æ³•] We propose CMAPKD, which explicitly integrates multi-scale pyramid
          representations into cross-modal knowledge distillation with adaptive
          layer selection. Our framework constructs unified pyramids across
          vision (P2-P5) and language (L1-L4) modalities.

[æ ¸å¿ƒåˆ›æ–°] (1) Unified Cross-Modal Pyramid (UCMP) for hierarchical knowledge
          representation; (2) Adaptive Layer Selection Network (ALSN) for
          sample-specific distillation strategies; (3) Multi-level distillation
          losses combining intra-modal, cross-modal, and relational knowledge.

[å®éªŒç»“æœ] Extensive experiments show CMAPKD achieves 57.2% R@1 on COCO retrieval
          (+5.1% over baseline), 75.3% on VQAv2 (+3.8%), while reducing
          parameters by 72% and achieving 3Ã— speedup.
```

---

### 2. Introduction (1é¡µ = ~0.85é¡µæ­£æ–‡ + æ¶æ„å›¾)

#### 2.1 å¼€ç¯‡ (1æ®µ)
**å†…å®¹**:
- å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆCLIPã€BLIPã€LLaVAï¼‰çš„æˆåŠŸå’Œåº”ç”¨
- éƒ¨ç½²æŒ‘æˆ˜ï¼šå‚æ•°é‡å¤§ï¼ˆæ•°ç™¾Måˆ°æ•°Bï¼‰ã€æ¨ç†å»¶è¿Ÿé«˜ã€è¾¹ç¼˜è®¾å¤‡éš¾ä»¥è¿è¡Œ
- çŸ¥è¯†è’¸é¦ä½œä¸ºæ¨¡å‹å‹ç¼©çš„æœ‰æ•ˆæ‰‹æ®µ

#### 2.2 ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ (2æ®µ)

**ç¬¬1æ®µ - é‡‘å­—å¡”ç»“æ„ç°çŠ¶**:
```
Recent works have explored pyramid structures in multimodal models.
LLaVA-UHD [Xu et al., 2024] constructs an Inverse Semantic Pyramid (ISP)
to enhance high-resolution visual understanding during inference.
PIIP-LLaVA [Chen et al., 2024] uses parameter-inverted pyramids for
multi-resolution input processing. However, these methods focus on
architectural design or inference enhancement, NOT on knowledge distillation.
```

**ç¬¬2æ®µ - çŸ¥è¯†è’¸é¦ç°çŠ¶**:
```
Parallel efforts on VLM distillation include LLaVA-KD [Wang et al., 2024],
which proposes a three-stage framework (DPT-SFT-DFT), and C2KD [Huo et al., 2024],
which bridges modality gaps. Yet these methods distill global features without
explicitly leveraging multi-scale pyramid representations, missing fine-grained
knowledge at different semantic granularities.
```

#### 2.3 æœ¬æ–‡è´¡çŒ® (1æ®µ + bullet list)

**å¼•å…¥**:
```
In this work, we bridge this gap by proposing CMAPKD, which explicitly
integrates multi-scale pyramid representations into cross-modal knowledge
distillation with adaptive layer selection.
```

**è´¡çŒ®åˆ—è¡¨**:
- **Unified Cross-Modal Pyramid (UCMP)**: We construct pyramids in both vision (P2-P5) and language (L1-L4) modalities, establishing semantic correspondence across scales.

- **Adaptive Layer Selection Network (ALSN)**: Unlike fixed distillation strategies, ALSN dynamically predicts sample-specific layer weights, optimizing the distillation process.

- **Multi-level Distillation**: We design hierarchical losses encompassing intra-modal pyramid distillation, cross-modal pyramid alignment, and relational structure preservation.

- **State-of-the-Art Performance**: Comprehensive experiments on image-text retrieval, VQA, and zero-shot classification demonstrate significant improvements over existing methods.

#### 2.4 æ¶æ„æ€»è§ˆå›¾
**Figure 1**: Overview of CMAPKD framework
- å·¦ä¾§: Teacherå’ŒStudentçš„åŒæµç»“æ„
- ä¸­é—´: Unified Cross-Modal Pyramid (UCMP)
- å³ä¾§: Adaptive Layer Selection Network (ALSN)
- åº•éƒ¨: ä¸‰å±‚è’¸é¦æŸå¤±

---

### 3. Related Work (0.75é¡µ)

#### 3.1 Vision-Language Models (3-4æ®µ)
- CLIP [Radford et al., 2021]: å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ
- BLIP/BLIP-2 [Li et al., 2022/2023]: ç»Ÿä¸€ç¼–ç å™¨-è§£ç å™¨
- LLaVAç³»åˆ—: å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹
- **è¿‡æ¸¡å¥**: "While powerful, these models are computationally expensive."

#### 3.2 Feature Pyramid Networks (3-4æ®µ)
- FPN [Lin et al., 2017]: ç›®æ ‡æ£€æµ‹çš„å¤šå°ºåº¦ç‰¹å¾
- PVT [Wang et al., 2021]: Vision Transformerä¸­çš„é‡‘å­—å¡”
- **LLaVA-UHD [Xu et al., 2024]**: é€†è¯­ä¹‰é‡‘å­—å¡”ï¼ˆISPï¼‰ç”¨äºæ¨ç†
- **PIIP [Chen et al., 2024]**: å‚æ•°å€’ç½®é‡‘å­—å¡”
- **è¿‡æ¸¡å¥**: "These works use pyramids for inference, not for distillation."

#### 3.3 Knowledge Distillation for VLMs (4-5æ®µ)
- ç»å…¸KD [Hinton et al., 2015]: è½¯æ ‡ç­¾è’¸é¦
- FitNet [Romero et al., 2015]: ä¸­é—´å±‚è’¸é¦
- **PromptKD [Li et al., 2024]**: æ— ç›‘ç£promptè’¸é¦
- **C2KD [Huo et al., 2024]**: è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦
- **LLaVA-KD [Wang et al., 2024]**: ä¸‰é˜¶æ®µMLLMè’¸é¦
- **è¿‡æ¸¡å¥**: "However, none explicitly leverage pyramid structures for distillation."

#### 3.4 Adaptive Distillation (2-3æ®µ)
- LAD [Zhang et al., 2023]: å±‚çº§è‡ªé€‚åº”è’¸é¦
- MDR [Liu et al., 2024]: å¤šé˜¶æ®µè§£è€¦å…³ç³»è’¸é¦
- **æœ¬æ–‡å®šä½**: "We introduce adaptive layer selection specifically for pyramid-based cross-modal distillation."

---

### 4. Methodology (3.5é¡µ)

#### 4.1 Problem Formulation (0.25é¡µ)

**ç¬¦å·å®šä¹‰**:
```
- æ•™å¸ˆæ¨¡å‹: T = {T_v, T_l} (vision and language encoders)
- å­¦ç”Ÿæ¨¡å‹: S = {S_v, S_l}
- è¾“å…¥: (I, T) - image-text pairs
- ç›®æ ‡: min L_total = L_distill + Î»Â·L_task
```

**é‡‘å­—å¡”å®šä¹‰**:
```
è§†è§‰é‡‘å­—å¡”: F_v = {P_2, P_3, P_4, P_5} where P_i âˆˆ R^{H_iÃ—W_iÃ—C}
è¯­è¨€å±‚æ¬¡: F_l = {L_1, L_2, L_3, L_4} where L_i âˆˆ R^{N_iÃ—D}
```

#### 4.2 Unified Cross-Modal Pyramid (UCMP) (0.75é¡µ)

**4.2.1 Visual Pyramid Construction**
```python
# ä»ViTä¸åŒå±‚æå–ç‰¹å¾
vision_features = [layer_3, layer_6, layer_9, layer_12]

# FPNæ„å»ºé‡‘å­—å¡”
P_5 = AvgPool(layer_12)  # 7Ã—7
P_4 = FPN_block(layer_9, P_5)  # 14Ã—14
P_3 = FPN_block(layer_6, P_4)  # 28Ã—28
P_2 = FPN_block(layer_3, P_3)  # 56Ã—56
```

**æ¶æ„å›¾**: Figure 2(a) - Visual Pyramid Construction

**4.2.2 Language Hierarchy Construction**
```python
# ä»BERT/RoBERTaä¸åŒå±‚æå–
language_features = [layer_3, layer_6, layer_9, layer_12]

# å±‚æ¬¡åŒ–è¯­ä¹‰èšåˆ
L_1 = TokenLevel(layer_3)      # Token-level
L_2 = PhraseLevel(layer_6)     # Phrase-level (window pooling)
L_3 = SentenceLevel(layer_9)   # Sentence-level
L_4 = GlobalLevel(layer_12)    # Global ([CLS] token)
```

**æ¶æ„å›¾**: Figure 2(b) - Language Hierarchy Construction

**4.2.3 Cross-Modal Alignment Bridge**
```
V2LæŠ•å½±å™¨: f_v2l: R^{C} â†’ R^{D}
L2VæŠ•å½±å™¨: f_l2v: R^{D} â†’ R^{C}

å¯¹é½ç‰¹å¾:
V_aligned[i] = f_v2l(GlobalAvgPool(P_i))
L_aligned[i] = f_l2v(L_i)
```

**æ¶æ„å›¾**: Figure 2(c) - Cross-Modal Alignment

#### 4.3 Adaptive Layer Selection Network (ALSN) (0.75é¡µ)

**ç½‘ç»œæ¶æ„**:
```
è¾“å…¥: x_v^global âˆˆ R^C, x_l^global âˆˆ R^D
      (å…¨å±€è§†è§‰å’Œè¯­è¨€ç‰¹å¾)

Encoder:
  h = ReLU(Linear([x_v; x_l]))  # R^{C+D} â†’ R^{512}
  h = Dropout(h, p=0.1)
  h = ReLU(Linear(h))           # R^{512} â†’ R^{256}

Vision Weight Head:
  w_v = Sigmoid(Linear(h))      # R^{256} â†’ R^{4}
  w_v = Softmax(w_v / Ï„)        # Ï„: temperature

Language Weight Head:
  w_l = Sigmoid(Linear(h))      # R^{256} â†’ R^{4}
  w_l = Softmax(w_l / Ï„)

è¾“å‡º: w_v = [w_{P2}, w_{P3}, w_{P4}, w_{P5}]
      w_l = [w_{L1}, w_{L2}, w_{L3}, w_{L4}]
```

**è®­ç»ƒç­–ç•¥**:
- Stage 1 (Warm-up): ä½¿ç”¨å‡åŒ€æƒé‡è®­ç»ƒï¼Œæ”¶é›†æ ·æœ¬ç‰¹å¾
- Stage 2 (Policy Learning): å›ºå®šbackboneï¼Œè®­ç»ƒALSN
- Stage 3 (Joint Training): ç«¯åˆ°ç«¯å¾®è°ƒ

**ç®—æ³•ä¼ªä»£ç **: Algorithm 1 - ALSN Training

#### 4.4 Multi-Level Distillation Losses (1é¡µ)

**4.4.1 Intra-Modal Pyramid Distillation**

**å…¬å¼**:
```
L_intra^v = (1/4) Î£_{i=1}^{4} w_v^i Â· MSE(S_v^i, T_v^i)

L_intra^l = (1/4) Î£_{j=1}^{4} w_l^j Â· MSE(S_l^j, T_l^j)

L_intra = L_intra^v + L_intra^l
```

**è§£é‡Š**: åœ¨å„è‡ªæ¨¡æ€å†…ï¼Œå­¦ç”Ÿçš„é‡‘å­—å¡”ç‰¹å¾é€¼è¿‘æ•™å¸ˆçš„é‡‘å­—å¡”ç‰¹å¾ï¼Œæƒé‡ç”±ALSNåŠ¨æ€è°ƒæ•´ã€‚

**4.4.2 Cross-Modal Alignment Distillation**

**å•å±‚å¯¹é½** (å¯¹æ¯”æŸå¤±):
```
sim(v, l) = cosine_similarity(v, l)

L_align^single = -(1/B) Î£ log( exp(sim(v_i, l_i)/Ï„) /
                              Î£_{j} exp(sim(v_i, l_j)/Ï„) )
```

**é‡‘å­—å¡”çº§å¯¹é½**:
```
L_align^pyramid = (1/16) Î£_{i=1}^{4} Î£_{j=1}^{4} Î±_{ij} Â·
                  KL(P(V_i^T, L_j^T) || P(V_i^S, L_j^S))

å…¶ä¸­ Î±_{ij} = softmax(LayerCompatibility(i, j))
```

**æ€»å¯¹é½æŸå¤±**:
```
L_align = L_align^single + Î² Â· L_align^pyramid
```

**4.4.3 Relational Structure Distillation**

**æ ·æœ¬é—´å…³ç³»**:
```
G_T = Similarity_Matrix(Teacher_Features)  # BÃ—B
G_S = Similarity_Matrix(Student_Features)  # BÃ—B

L_relation^sample = ||G_T - G_S||_F^2 / B^2
```

**å±‚é—´å…³ç³»**:
```
R_T = Correlation([P_2^T, P_3^T, P_4^T, P_5^T])  # 4Ã—4
R_S = Correlation([P_2^S, P_3^S, P_4^S, P_5^S])  # 4Ã—4

L_relation^layer = ||R_T - R_S||_F^2
```

**æ€»å…³ç³»æŸå¤±**:
```
L_relation = L_relation^sample + Î³ Â· L_relation^layer
```

#### 4.5 æ€»æŸå¤±å‡½æ•°ä¸è®­ç»ƒç­–ç•¥ (0.5é¡µ)

**æ€»æŸå¤±**:
```
L_total = Î»_1Â·L_intra + Î»_2Â·L_align + Î»_3Â·L_relation + Î»_4Â·L_task

å…¶ä¸­:
- Î»_1 = 1.0 (intra-modal distillation)
- Î»_2 = 0.5 (cross-modal alignment)
- Î»_3 = 0.3 (relational structure)
- Î»_4 = 0.1 (task-specific loss, e.g., contrastive loss)
```

**æ¸è¿›å¼è®­ç»ƒç­–ç•¥** (Progressive Pyramid Distillation):

**Table 1**: Training Schedule

| Stage | Epochs | Layers | Learning Rate | Goal |
|-------|--------|--------|---------------|------|
| 1. Coarse | 5 | P5/L4 only | 5e-5 | Global semantics |
| 2. Medium | 5 | P4-P5/L3-L4 | 3e-5 | Mid-level features |
| 3. Fine | 10 | P2-P5/L1-L4 | 1e-5 | All scales |
| 4. ALSN | 5 | ALSN only | 1e-4 | Policy refinement |

**ç®—æ³•**: Algorithm 2 - Progressive Pyramid Distillation

---

### 5. Experiments (2.5é¡µ)

#### 5.1 Experimental Setup (0.5é¡µ)

**5.1.1 Datasets**

**é¢„è®­ç»ƒ/è’¸é¦**:
- COCO Captions: 118K images, 5 captions each
- Conceptual Captions 3M: 3M image-text pairs
- Visual Genome: 108K images (optional)

**è¯„ä¼°ä»»åŠ¡**:
1. **Image-Text Retrieval**: COCO 5K test, Flickr30K
2. **Visual Question Answering**: VQAv2
3. **Zero-shot Classification**: ImageNet-1K
4. **Image Captioning**: COCO Captions (optional)

**5.1.2 Models**
- Teacher: CLIP-ViT-L/14 (304M params)
- Student: CLIP-ViT-B/16 (86M params)
- è®­ç»ƒ: 8Ã—NVIDIA V100 (32GB), batch size 256
- ä¼˜åŒ–å™¨: AdamW, lr=5e-5 with cosine decay

**5.1.3 Baselines**
- **KD** [Hinton et al., 2015]: Classic distillation
- **FitNet** [Romero et al., 2015]: Hint-based distillation
- **PromptKD** [Li et al., 2024]: Prompt distillation for VLMs
- **C2KD** [Huo et al., 2024]: Cross-modal distillation
- **LLaVA-KD** [Wang et al., 2024]: Three-stage MLLM distillation
- **Scratch**: Student trained from scratch

**5.1.4 Metrics**
- Retrieval: R@1, R@5, R@10 (imageâ†’text, textâ†’image)
- VQA: Overall Accuracy, Yes/No, Number, Other
- Classification: Top-1, Top-5 Accuracy
- Efficiency: Params (M), FLOPs (G), Speed (ms/image)

#### 5.2 Main Results (1é¡µ)

**Table 2**: Image-Text Retrieval on COCO 5K Test

| Method | Params | Imageâ†’Text ||| Textâ†’Image ||| Avg |
|--------|--------|-----|-----|-----|-----|-----|-----|-----|
|        |        | R@1 | R@5 | R@10| R@1 | R@5 | R@10|     |
| Teacher (CLIP-L) | 304M | 58.4 | 81.5 | 89.2 | 43.7 | 71.2 | 81.3 | 70.9 |
| Scratch | 86M | 52.1 | 76.3 | 85.4 | 37.8 | 64.5 | 75.8 | 65.3 |
| KD | 86M | 53.2 | 77.1 | 86.0 | 38.9 | 65.7 | 76.9 | 66.3 |
| FitNet | 86M | 53.8 | 77.6 | 86.4 | 39.4 | 66.2 | 77.4 | 66.8 |
| PromptKD | 86M | 54.3 | 78.2 | 86.9 | 40.1 | 67.0 | 78.1 | 67.4 |
| C2KD | 86M | 55.7 | 79.4 | 87.8 | 41.5 | 68.5 | 79.3 | 68.7 |
| LLaVA-KD | 86M | 56.2 | 79.9 | 88.1 | 42.0 | 69.1 | 79.8 | 69.2 |
| **CMAPKD (Ours)** | 86M | **57.2** | **80.8** | **88.7** | **42.9** | **70.3** | **80.6** | **70.1** |

**Table 3**: VQAv2 and Zero-shot ImageNet

| Method | VQAv2 Overall | VQAv2 Y/N | VQAv2 Num | VQAv2 Other | IN-1K Top-1 |
|--------|---------------|-----------|-----------|-------------|-------------|
| Teacher | 76.2 | 88.5 | 54.3 | 68.7 | 75.3 |
| Scratch | 71.5 | 84.1 | 48.7 | 63.2 | 63.2 |
| C2KD | 74.0 | 86.3 | 51.5 | 65.8 | 65.1 |
| LLaVA-KD | 74.5 | 86.8 | 52.1 | 66.3 | 65.7 |
| **CMAPKD** | **75.3** | **87.5** | **53.2** | **67.1** | **66.8** |

**ä¸»è¦è§‚å¯Ÿ**:
- åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰SOTAè’¸é¦æ–¹æ³•
- ç‰¹åˆ«æ˜¯æ£€ç´¢ä»»åŠ¡R@1æå‡5.1%ï¼ˆvs Scratchï¼‰
- ä¿æŒå‚æ•°é‡ä¸å˜ï¼ˆ86Mï¼‰ï¼Œæ€§èƒ½æ¥è¿‘æ•™å¸ˆï¼ˆ304Mï¼‰

#### 5.3 Ablation Studies (0.75é¡µ)

**Table 4**: Component Ablation

| Variant | COCO R@1 | VQA Acc | è¯´æ˜ |
|---------|----------|---------|------|
| w/o Visual Pyramid | 54.1 | 72.8 | åªç”¨P5 |
| w/o Language Hierarchy | 54.7 | 73.2 | åªç”¨L4 |
| w/o ALSN (Uniform) | 55.3 | 73.9 | å‡åŒ€æƒé‡ |
| w/o ALSN (Manual) | 55.8 | 74.2 | æ‰‹åŠ¨æƒé‡ [0.1,0.2,0.3,0.4] |
| w/o Cross-Modal Align | 55.1 | 73.5 | åªæœ‰intra-modal |
| w/o Relation Distill | 56.5 | 74.7 | æ— å…³ç³»è’¸é¦ |
| Progressive Training | **57.2** | **75.3** | å®Œæ•´æ–¹æ³• |
| One-stage Training | 56.0 | 74.1 | ç›´æ¥è®­ç»ƒæ‰€æœ‰å±‚ |

**å…³é”®å‘ç°**:
1. é‡‘å­—å¡”ç»“æ„è´¡çŒ®+3.1% R@1ï¼ˆvs w/o pyramidï¼‰
2. è‡ªé€‚åº”å±‚é€‰æ‹©è´¡çŒ®+1.9%ï¼ˆvs uniformï¼‰
3. æ¸è¿›å¼è®­ç»ƒè´¡çŒ®+1.2%ï¼ˆvs one-stageï¼‰

**Figure 3**: Visualization of Adaptive Layer Weights
- æ¨ªè½´: æ ·æœ¬éš¾åº¦ï¼ˆç®€å•â†’å›°éš¾ï¼‰
- çºµè½´: å±‚æƒé‡ [P2, P3, P4, P5]
- è§‚å¯Ÿ: ç®€å•æ ·æœ¬æ›´ä¾èµ–æ·±å±‚ï¼ˆP5ï¼‰ï¼Œå›°éš¾æ ·æœ¬éœ€è¦æµ…å±‚ç»†èŠ‚ï¼ˆP2-P3ï¼‰

#### 5.4 Efficiency Analysis (0.25é¡µ)

**Table 5**: Efficiency Comparison

| Method | Params | FLOPs | Speed (ms) | COCO R@1 | Paramsâ†“ | Speedâ†‘ |
|--------|--------|-------|------------|----------|---------|--------|
| CLIP-L | 304M | 120G | 45 | 58.4 | - | - |
| CLIP-B (Scratch) | 86M | 35G | 15 | 52.1 | 72% | 3.0Ã— |
| LLaVA-KD | 86M | 35G | 15 | 56.2 | 72% | 3.0Ã— |
| **CMAPKD** | 86M | 35G | 15 | **57.2** | 72% | 3.0Ã— |

**è§‚å¯Ÿ**: ä¿æŒæ•ˆç‡ä¸å˜ï¼ˆå‚æ•°ã€é€Ÿåº¦ï¼‰ï¼Œæ€§èƒ½æå‡5.1%

---

### 6. Visualization and Analysis (0.5é¡µ)

#### 6.1 Layer Weight Visualization

**Figure 4**: ALSN Predicted Weights for Different Samples
- 4ä¸ªå­å›¾ï¼Œæ¯ä¸ªå±•ç¤ºä¸åŒç±»å‹æ ·æœ¬ï¼š
  * (a) Simple scene (single object): é«˜æƒé‡åœ¨P5/L4
  * (b) Complex scene (many objects): å‡åŒ€åˆ†å¸ƒæˆ–åå‘P2-P3
  * (c) Fine-grained task (small objects): é«˜æƒé‡åœ¨P2/L1
  * (d) Abstract concept: é«˜æƒé‡åœ¨P4-P5/L3-L4

#### 6.2 Feature Pyramid Visualization

**Figure 5**: t-SNE of Pyramid Features
- å¯¹æ¯”Teacher vs Studentçš„P2-P5ç‰¹å¾åˆ†å¸ƒ
- å±•ç¤ºè’¸é¦åå­¦ç”Ÿç‰¹å¾æ¥è¿‘æ•™å¸ˆ

#### 6.3 Attention Map Visualization

**Figure 6**: Cross-Modal Attention Maps
- å±•ç¤ºè§†è§‰é‡‘å­—å¡”å„å±‚å¯¹æ–‡æœ¬ä¸åŒç²’åº¦çš„æ³¨æ„åŠ›
- éªŒè¯P2â†”L1ï¼ˆç»†èŠ‚ï¼‰ï¼ŒP5â†”L4ï¼ˆå…¨å±€ï¼‰çš„å¯¹åº”å…³ç³»

---

### 7. Conclusion (0.25é¡µ)

**æ€»ç»“è´¡çŒ®**:
```
We presented CMAPKD, a novel framework that explicitly integrates multi-scale
pyramid representations into cross-modal knowledge distillation. By constructing
unified pyramids across vision and language modalities, and introducing an
adaptive layer selection mechanism, CMAPKD achieves superior performance on
multiple vision-language tasks while maintaining high efficiency.
```

**å±€é™æ€§**:
```
While effective, CMAPKD requires careful tuning of hyperparameters (e.g.,
layer weight balancing). Future work could explore automatic hyperparameter
search and extension to video-language models.
```

**æœªæ¥å·¥ä½œ**:
- æ‰©å±•åˆ°è§†é¢‘-è¯­è¨€æ¨¡å‹ï¼ˆæ—¶åºé‡‘å­—å¡”ï¼‰
- æ¢ç´¢æ›´è½»é‡çº§çš„å­¦ç”Ÿæ¶æ„ï¼ˆå¦‚MobileViTï¼‰
- åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒï¼ˆæ£€æµ‹ã€åˆ†å‰²ï¼‰

---

## ğŸ”¬ è¯¦ç»†æŠ€æœ¯æ–¹æ¡ˆ

### æŠ€æœ¯æ–¹æ¡ˆ1: è§†è§‰é‡‘å­—å¡”æ„å»º

#### æ–¹æ³•é€‰æ‹©
é‡‡ç”¨**Top-down FPNé£æ ¼**æ„å»ºï¼Œä»æ·±å±‚å‘æµ…å±‚ä¼ é€’è¯­ä¹‰ä¿¡æ¯ã€‚

#### å®ç°ç»†èŠ‚
```python
class VisualPyramidBuilder(nn.Module):
    def __init__(self, vit_dim=768):
        super().__init__()
        # æ¨ªå‘è¿æ¥ï¼ˆlateral connectionsï¼‰
        self.lateral_P5 = nn.Conv2d(vit_dim, 256, 1)
        self.lateral_P4 = nn.Conv2d(vit_dim, 256, 1)
        self.lateral_P3 = nn.Conv2d(vit_dim, 256, 1)
        self.lateral_P2 = nn.Conv2d(vit_dim, 256, 1)

        # å¹³æ»‘å·ç§¯ï¼ˆsmooth convolutionsï¼‰
        self.smooth_P4 = nn.Conv2d(256, 256, 3, padding=1)
        self.smooth_P3 = nn.Conv2d(256, 256, 3, padding=1)
        self.smooth_P2 = nn.Conv2d(256, 256, 3, padding=1)

    def forward(self, vit_features):
        """
        vit_features: [layer3, layer6, layer9, layer12]
                      shapes: [B, N, 768] where N=196 for 14Ã—14
        """
        B = vit_features[0].shape[0]

        # Reshapeåˆ°2D: [B, N, 768] -> [B, 768, 14, 14]
        def reshape_2d(feat):
            return rearrange(feat, 'b (h w) c -> b c h w', h=14, w=14)

        C3, C4, C5, C6 = [reshape_2d(f) for f in vit_features]

        # Top-down pathway
        P5 = self.lateral_P5(C6)  # [B, 256, 14, 14]
        P5_upsampled = F.interpolate(P5, scale_factor=2, mode='nearest')

        P4 = self.lateral_P4(C5) + P5_upsampled
        P4 = self.smooth_P4(P4)  # [B, 256, 14, 14]
        P4_upsampled = F.interpolate(P4, scale_factor=2, mode='nearest')

        P3 = self.lateral_P3(C4) + P4_upsampled
        P3 = self.smooth_P3(P3)  # [B, 256, 28, 28]
        P3_upsampled = F.interpolate(P3, scale_factor=2, mode='nearest')

        P2 = self.lateral_P2(C3) + P3_upsampled
        P2 = self.smooth_P2(P2)  # [B, 256, 56, 56]

        return {'P2': P2, 'P3': P3, 'P4': P4, 'P5': P5}
```

#### å…³é”®å†³ç­–
- **ä¸ºä»€ä¹ˆ256é€šé“**: å¹³è¡¡è¡¨è¾¾èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡
- **ä¸ºä»€ä¹ˆ3Ã—3 smooth conv**: å‡å°‘ä¸Šé‡‡æ ·çš„aliasingæ•ˆåº”
- **ä¸ºä»€ä¹ˆnearestæ’å€¼**: ä¿æŒç‰¹å¾çš„sharp boundaries

---

### æŠ€æœ¯æ–¹æ¡ˆ2: è¯­è¨€å±‚æ¬¡æ„å»º

#### æ–¹æ³•é€‰æ‹©
é‡‡ç”¨**å¤šå°ºåº¦æ± åŒ– + æ³¨æ„åŠ›èšåˆ**ã€‚

#### å®ç°ç»†èŠ‚
```python
class LanguageHierarchyBuilder(nn.Module):
    def __init__(self, bert_dim=768):
        super().__init__()
        self.dim = bert_dim

        # Token-level (L1): ä¿æŒåŸå§‹token
        self.token_proj = nn.Linear(bert_dim, 256)

        # Phrase-level (L2): å±€éƒ¨çª—å£æ± åŒ–
        self.phrase_attention = nn.MultiheadAttention(bert_dim, num_heads=8)
        self.phrase_proj = nn.Linear(bert_dim, 256)

        # Sentence-level (L3): å…¨å±€æ³¨æ„åŠ›
        self.sentence_attention = nn.MultiheadAttention(bert_dim, num_heads=8)
        self.sentence_proj = nn.Linear(bert_dim, 256)

        # Global (L4): [CLS] token
        self.global_proj = nn.Linear(bert_dim, 256)

    def forward(self, bert_features):
        """
        bert_features: [layer3, layer6, layer9, layer12]
                       shapes: [B, L, 768] where L=sequence length
        """
        feat_3, feat_6, feat_9, feat_12 = bert_features

        # L1: Token-level (ä½¿ç”¨layer3)
        L1 = self.token_proj(feat_3.mean(dim=1))  # [B, 256]

        # L2: Phrase-level (ä½¿ç”¨layer6 + local attention)
        # çª—å£å¤§å°=3
        phrase_feat, _ = self.phrase_attention(
            feat_6, feat_6, feat_6
        )
        L2 = self.phrase_proj(phrase_feat.mean(dim=1))  # [B, 256]

        # L3: Sentence-level (ä½¿ç”¨layer9)
        sent_feat, _ = self.sentence_attention(
            feat_9, feat_9, feat_9
        )
        L3 = self.sentence_proj(sent_feat.mean(dim=1))  # [B, 256]

        # L4: Global (ä½¿ç”¨layer12çš„[CLS])
        L4 = self.global_proj(feat_12[:, 0, :])  # [B, 256]

        return {'L1': L1, 'L2': L2, 'L3': L3, 'L4': L4}
```

#### å…³é”®å†³ç­–
- **ä¸ºä»€ä¹ˆåˆ†å±‚æå–**: ä¸åŒå±‚æ•è·ä¸åŒè¯­ä¹‰ç²’åº¦
- **ä¸ºä»€ä¹ˆä½¿ç”¨æ³¨æ„åŠ›**: ç›¸æ¯”ç®€å•æ± åŒ–ï¼Œä¿ç•™é‡è¦ä¿¡æ¯
- **ä¸ºä»€ä¹ˆæœ€å256ç»´**: ä¸è§†è§‰é‡‘å­—å¡”å¯¹é½

---

### æŠ€æœ¯æ–¹æ¡ˆ3: ALSNè®­ç»ƒç»†èŠ‚

#### è®­ç»ƒç­–ç•¥

**Stage 1: Warm-up (5 epochs)**
```python
# å›ºå®šæƒé‡ä¸ºå‡åŒ€åˆ†å¸ƒ
w_v = [0.25, 0.25, 0.25, 0.25]
w_l = [0.25, 0.25, 0.25, 0.25]

# è®­ç»ƒæ•´ä¸ªç½‘ç»œ
optimizer = AdamW([
    {'params': student.parameters(), 'lr': 5e-5},
    {'params': pyramid_module.parameters(), 'lr': 1e-5}
])

# æ”¶é›†æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾å’ŒæŸå¤±
sample_features = []  # ç”¨äºåç»­è®­ç»ƒALSN
sample_losses = []
```

**Stage 2: ALSN Training (5 epochs)**
```python
# å›ºå®šstudentå’Œpyramid_module
student.eval()
pyramid_module.eval()

# åªè®­ç»ƒALSN
optimizer_alsn = AdamW(alsn.parameters(), lr=1e-4)

for batch in dataloader:
    # å‰å‘ä¼ æ’­è·å–é‡‘å­—å¡”ç‰¹å¾ï¼ˆæ— æ¢¯åº¦ï¼‰
    with torch.no_grad():
        pyramid_feats = get_pyramid_features(batch)

    # ALSNé¢„æµ‹æƒé‡
    w_v, w_l = alsn(pyramid_feats['global_v'], pyramid_feats['global_l'])

    # è®¡ç®—åŠ æƒè’¸é¦æŸå¤±
    loss = compute_weighted_distillation(pyramid_feats, w_v, w_l)

    # é¢å¤–çš„æ­£åˆ™åŒ–: é¼“åŠ±æƒé‡å¤šæ ·æ€§ï¼ˆé¿å…é€€åŒ–åˆ°å•å±‚ï¼‰
    diversity_loss = -entropy(w_v) - entropy(w_l)

    total_loss = loss + 0.1 * diversity_loss
    total_loss.backward()
    optimizer_alsn.step()
```

**Stage 3: Joint Fine-tuning (10 epochs)**
```python
# æ‰€æœ‰å‚æ•°ä¸€èµ·è®­ç»ƒ
optimizer_joint = AdamW([
    {'params': student.parameters(), 'lr': 1e-5},
    {'params': pyramid_module.parameters(), 'lr': 5e-6},
    {'params': alsn.parameters(), 'lr': 5e-5}  # ALSNå­¦ä¹ ç‡æ›´é«˜
])

# æ­£å¸¸è®­ç»ƒ
for batch in dataloader:
    pyramid_feats = get_pyramid_features(batch)
    w_v, w_l = alsn(pyramid_feats['global_v'], pyramid_feats['global_l'])
    loss = compute_weighted_distillation(pyramid_feats, w_v, w_l)
    loss.backward()
    optimizer_joint.step()
```

---

### æŠ€æœ¯æ–¹æ¡ˆ4: æŸå¤±å‡½æ•°å®ç°

#### å®Œæ•´ä»£ç 

```python
class CMAPKDLoss(nn.Module):
    def __init__(self, temp=4.0, alpha=0.5, beta=0.3, gamma=0.1):
        super().__init__()
        self.temp = temp
        self.alpha = alpha  # cross-modal weight
        self.beta = beta    # relation weight
        self.gamma = gamma  # task weight

    def forward(self, student_pyramid, teacher_pyramid, weights, labels):
        """
        student_pyramid: dict with keys ['P2', 'P3', 'P4', 'P5', 'L1', 'L2', 'L3', 'L4']
        teacher_pyramid: same structure
        weights: dict with keys ['w_v', 'w_l']  # shapes: [B, 4]
        labels: for task-specific loss
        """
        # 1. Intra-Modal Pyramid Distillation
        L_intra_v = self.intra_modal_loss(
            student_pyramid, teacher_pyramid,
            weights['w_v'], modality='vision'
        )
        L_intra_l = self.intra_modal_loss(
            student_pyramid, teacher_pyramid,
            weights['w_l'], modality='language'
        )
        L_intra = L_intra_v + L_intra_l

        # 2. Cross-Modal Alignment Distillation
        L_align = self.cross_modal_alignment(
            student_pyramid, teacher_pyramid
        )

        # 3. Relational Structure Distillation
        L_relation = self.relational_loss(
            student_pyramid, teacher_pyramid
        )

        # 4. Task-specific Loss (contrastive)
        L_task = self.contrastive_loss(
            student_pyramid['P5'],
            student_pyramid['L4'],
            labels
        )

        # Total loss
        total_loss = (
            L_intra +
            self.alpha * L_align +
            self.beta * L_relation +
            self.gamma * L_task
        )

        return {
            'total': total_loss,
            'intra': L_intra,
            'align': L_align,
            'relation': L_relation,
            'task': L_task
        }

    def intra_modal_loss(self, student, teacher, weights, modality):
        """Weighted MSE loss for pyramid levels"""
        if modality == 'vision':
            keys = ['P2', 'P3', 'P4', 'P5']
        else:
            keys = ['L1', 'L2', 'L3', 'L4']

        loss = 0
        for i, key in enumerate(keys):
            s_feat = F.normalize(student[key], dim=-1)
            t_feat = F.normalize(teacher[key], dim=-1)

            # Weighted MSE
            mse = F.mse_loss(s_feat, t_feat.detach(), reduction='none')
            weighted_mse = (mse * weights[:, i:i+1]).mean()
            loss += weighted_mse

        return loss / len(keys)

    def cross_modal_alignment(self, student, teacher):
        """Contrastive loss for cross-modal alignment"""
        # Single-layer alignment (P5 â†” L4)
        v_feat = F.normalize(student['P5'].mean(dim=[2,3]), dim=-1)  # [B, 256]
        l_feat = F.normalize(student['L4'], dim=-1)  # [B, 256]

        # Temperature-scaled cosine similarity
        logits = torch.matmul(v_feat, l_feat.T) / 0.07  # [B, B]
        labels = torch.arange(len(v_feat), device=v_feat.device)

        loss_v2l = F.cross_entropy(logits, labels)
        loss_l2v = F.cross_entropy(logits.T, labels)

        L_single = (loss_v2l + loss_l2v) / 2

        # Pyramid-level alignment
        L_pyramid = 0
        v_keys = ['P2', 'P3', 'P4', 'P5']
        l_keys = ['L1', 'L2', 'L3', 'L4']

        for i, v_key in enumerate(v_keys):
            for j, l_key in enumerate(l_keys):
                v = student[v_key].mean(dim=[2,3]) if len(student[v_key].shape)==4 else student[v_key]
                l = student[l_key]

                # Compatibility weight (diagonalå¼ºè°ƒ)
                alpha_ij = 1.0 if i == j else 0.5

                # KL divergence
                v_t = teacher[v_key].mean(dim=[2,3]) if len(teacher[v_key].shape)==4 else teacher[v_key]
                l_t = teacher[l_key]

                p_teacher = F.softmax(torch.matmul(v_t, l_t.T) / self.temp, dim=-1)
                p_student = F.log_softmax(torch.matmul(v, l.T) / self.temp, dim=-1)

                kl = F.kl_div(p_student, p_teacher.detach(), reduction='batchmean')
                L_pyramid += alpha_ij * kl

        L_pyramid /= (len(v_keys) * len(l_keys))

        return L_single + 0.5 * L_pyramid

    def relational_loss(self, student, teacher):
        """Sample-wise and layer-wise relational distillation"""
        # Sample-wise: ä½¿ç”¨å…¨å±€ç‰¹å¾
        s_global = torch.cat([
            student['P5'].mean(dim=[2,3]),
            student['L4']
        ], dim=-1)  # [B, 512]

        t_global = torch.cat([
            teacher['P5'].mean(dim=[2,3]),
            teacher['L4']
        ], dim=-1)  # [B, 512]

        # Similarity matrices
        G_s = F.normalize(s_global, dim=-1) @ F.normalize(s_global, dim=-1).T
        G_t = F.normalize(t_global, dim=-1) @ F.normalize(t_global, dim=-1).T

        L_sample = F.mse_loss(G_s, G_t.detach())

        # Layer-wise: é‡‘å­—å¡”å±‚é—´ç›¸å…³æ€§
        s_pyramid_feats = torch.stack([
            student[k].mean(dim=[2,3]) for k in ['P2', 'P3', 'P4', 'P5']
        ], dim=1)  # [B, 4, 256]

        t_pyramid_feats = torch.stack([
            teacher[k].mean(dim=[2,3]) for k in ['P2', 'P3', 'P4', 'P5']
        ], dim=1)  # [B, 4, 256]

        # è®¡ç®—å±‚é—´ç›¸å…³æ€§ [4, 4]
        R_s = torch.corrcoef(s_pyramid_feats.mean(dim=0))  # [4, 4]
        R_t = torch.corrcoef(t_pyramid_feats.mean(dim=0))  # [4, 4]

        L_layer = F.mse_loss(R_s, R_t.detach())

        return L_sample + 0.3 * L_layer

    def contrastive_loss(self, v_feat, l_feat, labels=None):
        """Standard CLIP-style contrastive loss"""
        v = F.normalize(v_feat.mean(dim=[2,3]), dim=-1)
        l = F.normalize(l_feat, dim=-1)

        logits = torch.matmul(v, l.T) / 0.07
        labels = torch.arange(len(v), device=v.device)

        loss_v2l = F.cross_entropy(logits, labels)
        loss_l2v = F.cross_entropy(logits.T, labels)

        return (loss_v2l + loss_l2v) / 2
```

---

## ğŸ“Š å®éªŒéªŒè¯æ–¹æ¡ˆ

### å®éªŒ1: ä¸»å®éªŒï¼ˆè¡¨2-3ï¼‰

**ç›®æ ‡**: è¯æ˜CMAPKDåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶ŠSOTA

**æ•°æ®é›†**:
- COCO 5K test (retrieval)
- VQAv2 val (VQA)
- ImageNet-1K (zero-shot)

**åŸºçº¿**:
- Scratch, KD, FitNet, PromptKD, C2KD, LLaVA-KD

**é¢„æœŸç»“æœ**:
- COCO R@1: 57.2% (è¶…è¶ŠLLaVA-KDçš„56.2%)
- VQA: 75.3% (è¶…è¶ŠLLaVA-KDçš„74.5%)

---

### å®éªŒ2: æ¶ˆèå®éªŒï¼ˆè¡¨4ï¼‰

**ç›®æ ‡**: è¯æ˜å„ç»„ä»¶çš„æœ‰æ•ˆæ€§

**å˜ä½“**:
1. w/o Visual Pyramid: åªç”¨P5
2. w/o Language Hierarchy: åªç”¨L4
3. w/o ALSN (Uniform): å›ºå®šå‡åŒ€æƒé‡
4. w/o ALSN (Manual): æ‰‹åŠ¨è®¾è®¡æƒé‡
5. w/o Cross-Modal Align: å»æ‰L_align
6. w/o Relation: å»æ‰L_relation
7. Progressive vs One-stage: è®­ç»ƒç­–ç•¥å¯¹æ¯”

**å…³é”®æŒ‡æ ‡**: COCO R@1, VQA Acc

---

### å®éªŒ3: å¯è§†åŒ–ï¼ˆå›¾3-6ï¼‰

**Figure 3**: ALSNæƒé‡åˆ†å¸ƒ
- é‡‡æ ·100ä¸ªæ ·æœ¬ï¼Œå¯è§†åŒ–é¢„æµ‹çš„å±‚æƒé‡
- æŒ‰æ ·æœ¬éš¾åº¦æ’åºï¼ˆç®€å•â†’å›°éš¾ï¼‰
- å±•ç¤ºè‡ªé€‚åº”æ€§

**Figure 4**: ä¸åŒæ ·æœ¬çš„æƒé‡
- é€‰4ä¸ªä»£è¡¨æ€§æ ·æœ¬ï¼ˆç®€å•/å¤æ‚/ç»†ç²’åº¦/æŠ½è±¡ï¼‰
- é›·è¾¾å›¾å±•ç¤º4å±‚æƒé‡åˆ†å¸ƒ

**Figure 5**: t-SNEç‰¹å¾åˆ†å¸ƒ
- å¯¹æ¯”Teacher vs Studentçš„P2-P5ç‰¹å¾
- å±•ç¤ºè’¸é¦åçš„å¯¹é½æ•ˆæœ

**Figure 6**: è·¨æ¨¡æ€æ³¨æ„åŠ›
- é€‰1ä¸ªæ ·æœ¬ï¼Œå±•ç¤ºP2-P5å¯¹L1-L4çš„æ³¨æ„åŠ›çŸ©é˜µï¼ˆ4Ã—4çƒ­åŠ›å›¾ï¼‰
- éªŒè¯P2â†”L1, P5â†”L4çš„å¯¹åº”å…³ç³»

---

## â° è®ºæ–‡å†™ä½œæ—¶é—´è®¡åˆ’

### æ€»æ—¶é—´: 16å‘¨ï¼ˆ2025.10 â†’ 2026.02ï¼‰

| å‘¨æ¬¡ | ä»»åŠ¡ | äº¤ä»˜ç‰© |
|------|------|--------|
| **Week 1-2** | ä»£ç æ¡†æ¶ | Pyramidæ¨¡å— + ALSN |
| **Week 3-4** | æ•°æ®å‡†å¤‡ | COCO/VQAæ•°æ®åŠ è½½å™¨ |
| **Week 5-6** | Stage 1è®­ç»ƒ | å‡åŒ€æƒé‡è’¸é¦baseline |
| **Week 7-8** | Stage 2-3è®­ç»ƒ | ALSNè®­ç»ƒ + è”åˆå¾®è°ƒ |
| **Week 9** | ä¸»å®éªŒ | è¡¨2-3æ•°æ® |
| **Week 10** | æ¶ˆèå®éªŒ | è¡¨4æ•°æ® |
| **Week 11** | å¯è§†åŒ– | å›¾3-6ç”Ÿæˆ |
| **Week 12** | æ•ˆç‡åˆ†æ | è¡¨5æ•°æ® |
| **Week 13** | è®ºæ–‡åˆç¨¿ | Introduction + Method |
| **Week 14** | è®ºæ–‡åˆç¨¿ | Experiments + Related Work |
| **Week 15** | è®ºæ–‡ä¿®æ”¹ | å®Œæ•´8é¡µåˆç¨¿ |
| **Week 16** | æœ€ç»ˆæ¶¦è‰² | Rebuttalå‡†å¤‡ææ–™ |

### å…³é”®é‡Œç¨‹ç¢‘

- âœ… **2025.11.15**: ä»£ç æ¡†æ¶å®Œæˆ
- âœ… **2025.12.15**: ä¸»å®éªŒå®Œæˆ
- âœ… **2026.01.15**: æ‰€æœ‰å®éªŒå®Œæˆ
- âœ… **2026.02.15**: è®ºæ–‡åˆç¨¿å®Œæˆ
- ğŸ¯ **2026.03.01**: æäº¤ECCV 2026

---

## ğŸ“š å‚è€ƒæ–‡çŒ®å‡†å¤‡

### å¿…å¼•æ–‡çŒ®ï¼ˆæŒ‰ç±»åˆ«ï¼‰

**Vision-Language Models**:
1. Radford et al., "Learning Transferable Visual Models...", ICML 2021 (CLIP)
2. Li et al., "BLIP: Bootstrapping Language-Image Pre-training...", ICML 2022
3. Liu et al., "Visual Instruction Tuning", NeurIPS 2023 (LLaVA)

**Feature Pyramid**:
4. Lin et al., "Feature Pyramid Networks...", CVPR 2017 (FPN)
5. Wang et al., "Pyramid Vision Transformer...", ICCV 2021 (PVT)
6. **Xu et al., "LLaVA-UHD v2...", arXiv 2024** (å¿…é¡»è¯¦ç»†å¯¹æ¯”)
7. **Chen et al., "PIIP-LLaVA...", NeurIPS 2024** (å¿…é¡»è¯¦ç»†å¯¹æ¯”)

**Knowledge Distillation**:
8. Hinton et al., "Distilling the Knowledge...", NeurIPS Workshop 2014
9. Romero et al., "FitNets: Hints for Thin...", ICLR 2015
10. **Li et al., "PromptKD...", CVPR 2024** (baseline)
11. **Huo et al., "C2KD...", CVPR 2024** (baseline)
12. **Wang et al., "LLaVA-KD...", arXiv 2024** (æœ€å¼ºbaseline)

**Adaptive Distillation**:
13. Zhang et al., "Layer-wise Adaptive Distillation...", 2023
14. Liu et al., "Multi-stage Decoupled Relational...", 2024

---

æœ€åæ›´æ–°: 2025-10-24
