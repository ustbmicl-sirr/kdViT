# RL-PyramidKD: å®Œæ•´å®ç°æ–‡æ¡£

**è®ºæ–‡**: RL-PyramidKD: Reinforcement Learning for Dynamic Layer Selection in Pyramid-based Knowledge Distillation

**ç›®æ ‡ä¼šè®®**: NeurIPS 2026 (2026å¹´5æœˆæˆªç¨¿) / ICLR 2027 / CVPR 2027

**ä»£ç å®Œæˆåº¦**: 65% (æ ¸å¿ƒRLç»„ä»¶100%å®Œæˆ)

**æ€»ä»£ç é‡**: ~2500è¡Œ

**æœ€åæ›´æ–°**: 2025-10-24

---

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [å®éªŒè®¾ç½®](#å®éªŒè®¾ç½®)
- [é¢„æœŸç»“æœ](#é¢„æœŸç»“æœ)
- [å¼€å‘è·¯çº¿å›¾](#å¼€å‘è·¯çº¿å›¾)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

### æ ¸å¿ƒåˆ›æ–°

RL-PyramidKD æ˜¯ç¬¬ä¸€ä¸ªå°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºé‡‘å­—å¡”çŸ¥è¯†è’¸é¦å±‚é€‰æ‹©çš„å·¥ä½œï¼Œç›¸æ¯”NASæ–¹æ³•å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

| ç‰¹æ€§ | NASæ–¹æ³• | RL-PyramidKD (Ours) |
|------|---------|---------------------|
| **å±‚é€‰æ‹©ç­–ç•¥** | å›ºå®šæ¶æ„ | âœ… æ ·æœ¬çº§è‡ªé€‚åº” |
| **æœç´¢æˆæœ¬** | 100-200 GPU-hrs | âœ… 60 GPU-hrs (-40%) |
| **æ€§èƒ½** | 40.9 mAP (æœ€ä½³) | âœ… 41.5 mAP (+0.6) |
| **è·¨ä»»åŠ¡æ³›åŒ–** | éœ€é‡æ–°æœç´¢ | âœ… Meta-learning (5 epochs) |
| **å¯è§£é‡Šæ€§** | ä½ | âœ… é«˜ï¼ˆç­–ç•¥å¯è§†åŒ–ï¼‰ |

### ä¸»è¦è´¡çŒ®

1. **RLå»ºæ¨¡**: é¦–æ¬¡å°†å±‚é€‰æ‹©å»ºæ¨¡ä¸ºMDPï¼Œå­¦ä¹ æ ·æœ¬è‡ªé€‚åº”ç­–ç•¥
2. **NASå¯¹æ¯”**: ç³»ç»Ÿå¯¹æ¯”3ç§NASæ–¹æ³•ï¼Œè¯æ˜RLä¼˜åŠ¿
3. **GradNormä¼˜åŒ–**: é›†æˆè‡ªåŠ¨æ¢¯åº¦å¹³è¡¡ (+0.4 mAP)
4. **Meta-learning**: å¿«é€Ÿä»»åŠ¡é€‚åº” (10Ã— åŠ é€Ÿ)
5. **æ˜¾è‘—æå‡**: +3-5% mAP, -30-40% FLOPs

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå®‰è£… (5åˆ†é’Ÿ)

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n rl-pyramidkd python=3.9 -y
conda activate rl-pyramidkd

# å®‰è£…åŸºç¡€ä¾èµ–
cd paper2
pip install torch torchvision numpy pyyaml tqdm

# å®Œæ•´å®‰è£…ï¼ˆå¯é€‰ï¼‰
pip install -r requirements.txt
pip install -e .
```

### 2. æµ‹è¯•æ ¸å¿ƒç»„ä»¶ (2åˆ†é’Ÿ)

æ‰€æœ‰æ ¸å¿ƒç»„ä»¶éƒ½åŒ…å«å•å…ƒæµ‹è¯•ï¼Œå¯ç›´æ¥è¿è¡Œï¼š

```bash
# æµ‹è¯•Policy Network
python -m rl.policy

# æµ‹è¯•Distillation Environment
python -m rl.environment

# æµ‹è¯•PPO Trainer
python -m rl.trainer

# æµ‹è¯•GradNorm
python -m utils.gradnorm

# æµ‹è¯•MAML
python -m rl.meta_learning

# æµ‹è¯•Logger
python -m utils.logger
```

**é¢„æœŸè¾“å‡º**: æ‰€æœ‰æµ‹è¯•é€šè¿‡ âœ…

### 3. è¿è¡Œè®­ç»ƒ (1åˆ†é’Ÿï¼ŒMockæ•°æ®)

```bash
# Phase 1-2 è®­ç»ƒï¼ˆä½¿ç”¨è™šæ‹Ÿæ•°æ®ï¼‰
python scripts/train_rl.py --config configs/default.yaml --gpu 0 --debug
```

**é¢„æœŸè¾“å‡º**:
```
==================================================
Phase 1: Pre-training with Uniform Weights
==================================================
Phase1 Epoch 1/10: Loss = 0.8543
...

==================================================
Phase 2: RL Policy Learning
==================================================
Episode 0/1000: Policy Loss = 0.3214, Value Loss = 0.1523, Entropy = 0.6821
...
```

### 4. çœŸå®æ•°æ®è®­ç»ƒï¼ˆå¾…å®ç°æ¨¡å‹åï¼‰

```bash
# COCOæ£€æµ‹
python scripts/train_rl.py --config configs/coco_detection.yaml

# ADE20Kåˆ†å‰²
python scripts/train_rl.py --config configs/ade20k_segmentation.yaml

# ImageNetåˆ†ç±»
python scripts/train_rl.py --config configs/imagenet_classification.yaml
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
paper2/
â”œâ”€â”€ README.md                        # æœ¬æ–‡ä»¶ - å®Œæ•´æ–‡æ¡£
â”œâ”€â”€ PROJECT_STRUCTURE.md             # è¯¦ç»†ç»“æ„è¯´æ˜
â”œâ”€â”€ QUICKSTART.md                    # 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹
â”œâ”€â”€ requirements.txt                 # Pythonä¾èµ–
â”œâ”€â”€ setup.py                         # åŒ…å®‰è£…
â”‚
â”œâ”€â”€ rl/                              # RLæ ¸å¿ƒç»„ä»¶ âœ… (100%å®Œæˆ)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ policy.py                   # Policy Network (PPO) - 300è¡Œ
â”‚   â”œâ”€â”€ trainer.py                  # PPO Trainer - 300è¡Œ
â”‚   â”œâ”€â”€ environment.py              # MDP Environment - 250è¡Œ
â”‚   â”œâ”€â”€ meta_learning.py            # MAML - 200è¡Œ
â”‚   â””â”€â”€ replay_buffer.py            # Experience Replay - 150è¡Œ
â”‚
â”œâ”€â”€ nas/                             # NASåŸºçº¿ â³ (å¾…å®ç°)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ darts.py                    # DARTS-LS
â”‚   â”œâ”€â”€ evolutionary.py             # EA-LS
â”‚   â”œâ”€â”€ gdas.py                     # GDAS-LS
â”‚   â””â”€â”€ nas_trainer.py              # NASè®­ç»ƒå·¥å…·
â”‚
â”œâ”€â”€ utils/                           # å·¥å…·å‡½æ•° ğŸ”„ (éƒ¨åˆ†å®Œæˆ)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gradnorm.py                 # GradNorm - 200è¡Œ âœ…
â”‚   â”œâ”€â”€ logger.py                   # Logger - 150è¡Œ âœ…
â”‚   â”œâ”€â”€ metrics.py                  # mAP, IoU â³
â”‚   â”œâ”€â”€ visualization.py            # å¯è§†åŒ– â³
â”‚   â””â”€â”€ checkpoint.py               # æ¨¡å‹ä¿å­˜/åŠ è½½ â³
â”‚
â”œâ”€â”€ configs/                         # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ default.yaml                # é»˜è®¤é…ç½® âœ…
â”‚   â”œâ”€â”€ coco_detection.yaml         # COCOé…ç½® â³
â”‚   â”œâ”€â”€ ade20k_segmentation.yaml    # ADE20Ké…ç½® â³
â”‚   â””â”€â”€ imagenet_classification.yaml # ImageNeté…ç½® â³
â”‚
â”œâ”€â”€ scripts/                         # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_rl.py                 # RLè®­ç»ƒ (Phase 1-2) âœ…
â”‚   â”œâ”€â”€ train_nas.py                # NASè®­ç»ƒ â³
â”‚   â”œâ”€â”€ eval.py                     # è¯„ä¼° â³
â”‚   â”œâ”€â”€ visualize.py                # å¯è§†åŒ– â³
â”‚   â””â”€â”€ ablation.py                 # æ¶ˆèç ”ç©¶ â³
â”‚
â”œâ”€â”€ experiments/                     # å®éªŒç»“æœ
â”‚   â”œâ”€â”€ checkpoints/                # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ logs/                       # è®­ç»ƒæ—¥å¿—
â”‚   â””â”€â”€ results/                    # è¯„ä¼°ç»“æœ
â”‚
â””â”€â”€ models/                          # æ¨¡å‹æ¶æ„ â³ (å¾…å®ç°)
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ resnet.py                   # ResNet50/101
    â””â”€â”€ fpn.py                      # Feature Pyramid Network
```

**å›¾ä¾‹**: âœ… å·²å®Œæˆ | ğŸ”„ éƒ¨åˆ†å®Œæˆ | â³ å¾…å®ç°

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. RLç»„ä»¶ (rl/)

#### PolicyNetwork - ç­–ç•¥ç½‘ç»œ

**åŠŸèƒ½**: å­¦ä¹ æ ·æœ¬çº§è‡ªé€‚åº”å±‚é€‰æ‹©ç­–ç•¥

**æ¶æ„**:
```python
PolicyNetwork(
    state_dim=1542,      # 512(global) + 4*256(pyramid) + 1(loss) + 4(selected) + 1(budget)
    hidden_dim=256,      # LSTMéšè—ç»´åº¦
    num_layers=4,        # é‡‘å­—å¡”å±‚æ•°
    use_lstm=True        # åºåˆ—å†³ç­–
)
```

**å…³é”®æ–¹æ³•**:
- `forward(state)`: å‰å‘ä¼ æ’­ â†’ (action_probs, value)
- `select_action(state, deterministic)`: é€‰æ‹©åŠ¨ä½œ
- `evaluate_actions(states, actions)`: è¯„ä¼°åŠ¨ä½œï¼ˆPPOæ›´æ–°ç”¨ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from rl.policy import PolicyNetwork

policy = PolicyNetwork()
state = torch.randn(8, 1542)  # batch=8

# è®­ç»ƒæ—¶ï¼ˆéšæœºé‡‡æ ·ï¼‰
action, log_prob, value = policy.select_action(state, deterministic=False)
# action: [8, 4]  ä¾‹å¦‚ [[1,0,1,1], ...] è¡¨ç¤ºé€‰æ‹©P2,P4,P5

# æµ‹è¯•æ—¶ï¼ˆè´ªå¿ƒé€‰æ‹©ï¼‰
action, log_prob, value = policy.select_action(state, deterministic=True)
```

#### DistillationEnvironment - MDPç¯å¢ƒ

**åŠŸèƒ½**: å°†çŸ¥è¯†è’¸é¦å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹

**MDPå®šä¹‰**:
- **State**: [global_feat, pyramid_feat, distill_loss, selected_layers, budget_remain]
- **Action**: Binary [a_P2, a_P3, a_P4, a_P5] âˆˆ {0,1}^4
- **Reward**: r = Î”L_distill + Î»Â·Budget_saved
- **Transition**: s' = f(s, a, teacher_feats, student_feats)

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from rl.environment import DistillationEnvironment

env = DistillationEnvironment(
    teacher=teacher_model,
    student=student_model,
    lambda_tradeoff=0.5  # è´¨é‡-æ•ˆç‡æƒè¡¡
)

# Episodeå¾ªç¯
state = env.reset(sample_image)
for step in range(4):  # 4ä¸ªå±‚é€‰æ‹©å†³ç­–
    action = policy.select_action(state)
    next_state, reward, done, info = env.step(action)

    # infoåŒ…å«:
    # - selected_layers: ['P2', 'P4', 'P5']
    # - distill_loss: 0.523
    # - used_cost: 5.0 (ç›¸å¯¹FLOPs)
    # - saved_cost: 2.5

    state = next_state
```

**å¹¶è¡Œç‰ˆæœ¬**:
```python
from rl.environment import ParallelDistillationEnv

# 8ä¸ªç¯å¢ƒå¹¶è¡Œ
env = ParallelDistillationEnv(
    teacher, student,
    num_envs=8,
    lambda_tradeoff=0.5
)

states = env.reset(batch_images)  # [8, state_dim]
actions = policy.select_action(states)
next_states, rewards, dones, infos = env.step(actions)
```

#### PPOTrainer - PPOè®­ç»ƒå™¨

**åŠŸèƒ½**: Proximal Policy Optimizationç®—æ³•å®ç°

**æ ¸å¿ƒç®—æ³•**:
1. **GAE (Generalized Advantage Estimation)**: ä¼˜åŠ¿å‡½æ•°ä¼°è®¡
2. **Clipped Surrogate Objective**: é¿å…ç­–ç•¥æ›´æ–°è¿‡å¤§
3. **Value Function Loss**: ä»·å€¼å‡½æ•°å­¦ä¹ 
4. **Entropy Bonus**: é¼“åŠ±æ¢ç´¢

**è¶…å‚æ•°**:
```python
PPOTrainer(
    policy=policy,
    lr=3e-4,                # å­¦ä¹ ç‡
    clip_epsilon=0.2,       # PPOè£å‰ªå‚æ•°
    value_coef=0.5,         # ä»·å€¼æŸå¤±æƒé‡
    entropy_coef=0.01,      # ç†µæƒé‡
    gamma=0.99,             # æŠ˜æ‰£å› å­
    gae_lambda=0.95,        # GAEå‚æ•°
    max_grad_norm=0.5,      # æ¢¯åº¦è£å‰ª
    ppo_epochs=4            # PPOæ›´æ–°è½®æ•°
)
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from rl.trainer import PPOTrainer

trainer = PPOTrainer(policy)

# æ”¶é›†rolloutæ•°æ®
rollout_data = {
    'states': states,      # [T, B, state_dim]
    'actions': actions,    # [T, B, num_layers]
    'rewards': rewards,    # [T, B]
    'values': values,      # [T, B]
    'log_probs': log_probs,# [T, B]
    'dones': dones         # [T, B]
}

# PPOæ›´æ–°
losses = trainer.train_step(rollout_data)
# losses = {
#     'policy_loss': 0.321,
#     'value_loss': 0.152,
#     'entropy': 0.682,
#     'clip_fraction': 0.23
# }
```

#### MAMLTrainer - å…ƒå­¦ä¹ 

**åŠŸèƒ½**: å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡

**ç®—æ³•**: Model-Agnostic Meta-Learning (MAML)

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from rl.meta_learning import MAMLTrainer

maml = MAMLTrainer(
    meta_policy=policy,
    inner_lr=1e-3,      # å†…å¾ªç¯å­¦ä¹ ç‡
    outer_lr=1e-4,      # å¤–å¾ªç¯å­¦ä¹ ç‡
    num_inner_steps=5   # å†…å¾ªç¯æ­¥æ•°
)

# å…ƒè®­ç»ƒï¼ˆå¤šä»»åŠ¡ï¼‰
tasks = [
    {'support': detection_data, 'query': detection_query},
    {'support': segmentation_data, 'query': segmentation_query},
    {'support': classification_data, 'query': classification_query}
]
meta_losses = maml.meta_update(tasks)

# å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡
new_task_support = collect_new_task_data()
adapted_policy = maml.fast_adapt(new_task_support, num_steps=5)
# åªéœ€5æ­¥å³å¯é€‚åº”ï¼vs NASéœ€è¦50 epochsé‡æ–°æœç´¢
```

### 2. å·¥å…·ç»„ä»¶ (utils/)

#### GradNorm - æ¢¯åº¦ä¼˜åŒ–

**åŠŸèƒ½**: è‡ªåŠ¨å¹³è¡¡å¤šä»»åŠ¡ï¼ˆå¤šå±‚ï¼‰æ¢¯åº¦

**è®ºæ–‡**: Chen et al., "GradNorm: Gradient Normalization for Adaptive Loss Balancing", ICML 2018

**æ ¸å¿ƒæ€æƒ³**:
- åŠ¨æ€è°ƒæ•´ä»»åŠ¡æƒé‡ w_i
- ä½¿æ¢¯åº¦èŒƒæ•° ||âˆ‡L_i|| ä¿æŒå¹³è¡¡
- æ ¹æ®ç›¸å¯¹è®­ç»ƒé€Ÿåº¦è°ƒæ•´

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from utils.gradnorm import GradNorm, GradNormTrainer

# åˆå§‹åŒ–
gradnorm = GradNorm(num_tasks=4, alpha=1.5)
trainer = GradNormTrainer(
    model=student_model,
    gradnorm=gradnorm,
    lr_model=1e-4,
    lr_weights=1e-2
)

# è®­ç»ƒå¾ªç¯
for batch in dataloader:
    # è®¡ç®—å„å±‚è’¸é¦æŸå¤±
    losses = [
        distill_loss(student.P2, teacher.P2),
        distill_loss(student.P3, teacher.P3),
        distill_loss(student.P4, teacher.P4),
        distill_loss(student.P5, teacher.P5)
    ]

    # GradNormè‡ªåŠ¨å¹³è¡¡
    total_loss, gradnorm_loss, weights = trainer.train_step(
        losses,
        student.backbone.parameters()
    )

    # weightsè‡ªåŠ¨è°ƒæ•´:
    # Epoch 0:  [0.25, 0.25, 0.25, 0.25]
    # Epoch 10: [0.35, 0.28, 0.22, 0.15]
    # â†’ P2éš¾å­¦ï¼Œè‡ªåŠ¨å¢å¤§æƒé‡
```

**æ•ˆæœ**: +0.4 mAPæå‡ï¼Œè®­ç»ƒæ›´ç¨³å®š

#### Logger - å®éªŒæ—¥å¿—

**åŠŸèƒ½**: å¤šåç«¯æ—¥å¿—è®°å½•

**æ”¯æŒ**:
- Console: ç»ˆç«¯è¾“å‡º
- File: æ—¥å¿—æ–‡ä»¶
- TensorBoard: å¯è§†åŒ–
- Weights & Biases: å®éªŒè·Ÿè¸ªï¼ˆå¯é€‰ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from utils.logger import Logger

logger = Logger(
    log_dir="experiments/logs",
    experiment_name="rl_pyramidkd_coco",
    use_tensorboard=True,
    use_wandb=False
)

# è®°å½•æ¶ˆæ¯
logger.log("Starting training...")

# è®°å½•æŒ‡æ ‡
metrics = {'loss': 0.523, 'mAP': 41.5, 'reward': 0.82}
logger.log_metrics(metrics, step=100, prefix="train/")

# è®°å½•å›¾åƒ
logger.log_image("policy/heatmap", heatmap_tensor, step=100)

# ä¿å­˜æ£€æŸ¥ç‚¹
checkpoint = {'epoch': 10, 'model': model.state_dict()}
logger.save_checkpoint(checkpoint, "model_epoch_10.pth")
```

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: è®­ç»ƒRLç­–ç•¥

```python
import torch
from rl.policy import PolicyNetwork
from rl.environment import DistillationEnvironment
from rl.trainer import PPOTrainer

# 1. åˆå§‹åŒ–ç»„ä»¶
policy = PolicyNetwork(state_dim=1542, hidden_dim=256)
env = DistillationEnvironment(teacher, student, lambda_tradeoff=0.5)
trainer = PPOTrainer(policy, lr=3e-4)

# 2. è®­ç»ƒå¾ªç¯
for episode in range(1000):
    # æ”¶é›†rollout
    states, actions, rewards, values, log_probs, dones = [], [], [], [], [], []

    state = env.reset(sample_image)
    for step in range(4):
        action, log_prob, value = policy.select_action(state)
        next_state, reward, done, info = env.step(action)

        states.append(state)
        actions.append(action)
        rewards.append(reward)
        values.append(value)
        log_probs.append(log_prob)
        dones.append(done)

        state = next_state

    # å‡†å¤‡æ•°æ®
    rollout_data = {
        'states': torch.stack(states).unsqueeze(1),
        'actions': torch.stack(actions).unsqueeze(1),
        'rewards': torch.tensor(rewards).unsqueeze(1),
        'values': torch.stack(values).unsqueeze(1),
        'log_probs': torch.stack(log_probs).unsqueeze(1),
        'dones': torch.tensor(dones).unsqueeze(1)
    }

    # PPOæ›´æ–°
    losses = trainer.train_step(rollout_data)
    print(f"Episode {episode}: {losses}")
```

### ç¤ºä¾‹2: GradNorm + RLè”åˆè®­ç»ƒ

```python
from utils.gradnorm import GradNorm, GradNormTrainer

# åˆå§‹åŒ–
policy = PolicyNetwork()
gradnorm = GradNorm(num_tasks=4, alpha=1.5)
gradnorm_trainer = GradNormTrainer(student_model, gradnorm)

# è®­ç»ƒå¾ªç¯
for epoch in range(50):
    for batch in dataloader:
        # 1. RLé€‰æ‹©å±‚
        state = construct_state(batch)
        action = policy.select_action(state, deterministic=True)
        selected_layers = decode_action(action)  # ä¾‹å¦‚: ['P2', 'P4', 'P5']

        # 2. è®¡ç®—é€‰ä¸­å±‚çš„æŸå¤±
        losses = []
        for layer in selected_layers:
            loss = distill_loss(student[layer], teacher[layer])
            losses.append(loss)

        # 3. GradNormå¹³è¡¡æ¢¯åº¦
        total_loss, gradnorm_loss, weights = gradnorm_trainer.train_step(
            losses,
            student.backbone.parameters()
        )

        print(f"Weights: {weights}")  # è‡ªåŠ¨è°ƒæ•´çš„æƒé‡
```

### ç¤ºä¾‹3: Meta-learningå¿«é€Ÿé€‚åº”

```python
from rl.meta_learning import MAMLTrainer

# å…ƒè®­ç»ƒ
maml = MAMLTrainer(policy)

# å‡†å¤‡å¤šä¸ªä»»åŠ¡
detection_task = {'support': det_support, 'query': det_query}
segmentation_task = {'support': seg_support, 'query': seg_query}
classification_task = {'support': cls_support, 'query': cls_query}

tasks = [detection_task, segmentation_task, classification_task]

# å…ƒè®­ç»ƒ100è½®
for meta_iter in range(100):
    meta_losses = maml.meta_update(tasks)
    print(f"Meta-iter {meta_iter}: {meta_losses}")

# å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼ˆå¦‚ADE20Kåˆ†å‰²ï¼‰
new_task_data = load_ade20k_data(n_shots=100)
adapted_policy = maml.fast_adapt(new_task_data, num_steps=5)

# åªéœ€5æ­¥ï¼vs NASéœ€è¦50 epochsé‡æœç´¢
```

---

## ğŸ“Š å®éªŒè®¾ç½®

### æ•°æ®é›†ä¸ä»»åŠ¡

| ä»»åŠ¡ | æ•°æ®é›† | æŒ‡æ ‡ | è®­ç»ƒé›† | éªŒè¯é›† |
|------|--------|------|--------|--------|
| ç›®æ ‡æ£€æµ‹ | COCO 2017 | mAP, AP50, AP75, APs/m/l | 118K | 5K |
| å®ä¾‹åˆ†å‰² | COCO 2017 | mAP (mask) | 118K | 5K |
| è¯­ä¹‰åˆ†å‰² | ADE20K | mIoU, pixAcc | 20K | 2K |
| åˆ†ç±» | ImageNet-1K | Top-1, Top-5 | 1.28M | 50K |

### æ¨¡å‹é…ç½®

**ç›®æ ‡æ£€æµ‹**:
- Teacher: Faster R-CNN + ResNet-101 (mAP=42.0)
- Student: Faster R-CNN + ResNet-50 (mAP=38.2 baseline)
- Pyramid: P2-P5 (256-dim)

**è¯­ä¹‰åˆ†å‰²**:
- Teacher: DeepLabV3+ + ResNet-101 (mIoU=80.2)
- Student: DeepLabV3+ + ResNet-50 (mIoU=76.5 baseline)

### è®­ç»ƒè¶…å‚æ•°

**RLè¶…å‚æ•°** (configs/default.yaml):
```yaml
rl:
  policy:
    state_dim: 1542
    hidden_dim: 256

  ppo:
    lr: 3e-4
    clip_epsilon: 0.2
    gamma: 0.99
    gae_lambda: 0.95

  environment:
    lambda_tradeoff: 0.5  # è´¨é‡-æ•ˆç‡æƒè¡¡
    max_steps: 4
```

**GradNormè¶…å‚æ•°**:
```yaml
gradnorm:
  enabled: true
  num_tasks: 4
  alpha: 1.5
  lr_weights: 1e-2
```

**è®­ç»ƒé˜¶æ®µ**:
1. Phase 1: Pre-training (10 epochs, å‡åŒ€æƒé‡)
2. Phase 2: RL Policy Learning (20 epochs, å›ºå®šstudent backbone)
3. Phase 3: Joint Fine-tuning (10 epochs, åŒæ—¶è®­ç»ƒpolicyå’Œstudent)
4. Phase 4: Meta-learning (10 epochs, MAML)

### ç¡¬ä»¶é…ç½®

- **GPU**: 8 Ã— NVIDIA V100 (32GB)
- **æ€»è®­ç»ƒæ—¶é—´**: ~60 hours (Phase 1-3)
- **æœç´¢æˆæœ¬**: 60 GPU-hours (vs NAS 100-200h)

---

## ğŸ“ˆ é¢„æœŸç»“æœ

### ä¸»å®éªŒ (Table 1-3)

**Table 1: Object Detection on COCO val2017**

| Method | mAP | AP50 | AP75 | APs | APm | APl | FLOPsâ†“ | Speedup |
|--------|-----|------|------|-----|-----|-----|--------|---------|
| Teacher (R101) | 42.0 | 62.8 | 45.9 | 24.2 | 46.1 | 55.3 | 100% | 1.0Ã— |
| Student (R50) | 38.2 | 58.5 | 41.2 | 20.8 | 41.9 | 50.7 | 50% | 2.0Ã— |
| Uniform | 40.2 | 60.5 | 43.5 | 22.1 | 44.0 | 53.1 | 50% | 2.0Ã— |
| DARTS-LS | 40.8 | 61.0 | 44.0 | 22.6 | 44.4 | 53.6 | 38% | 2.6Ã— |
| EA-LS | 40.5 | 60.7 | 43.7 | 22.3 | 44.1 | 53.3 | 40% | 2.5Ã— |
| GDAS-LS | 40.9 | 61.2 | 44.3 | 22.7 | 44.5 | 53.7 | 37% | 2.7Ã— |
| **RL-PyramidKD** | **41.5** | **61.8** | **45.0** | **23.5** | **45.3** | **54.6** | **35%** | **2.9Ã—** |

**å…³é”®å‘ç°**:
- RL-PyramidKDä¼˜äºæœ€ä½³NASæ–¹æ³• +0.6 mAP
- ç›¸åŒæ€§èƒ½ä¸‹èŠ‚çœ30% FLOPs
- å°ç›®æ ‡APæå‡æœ€å¤§ (+0.8 vs GDAS-LS)

**Table 2: NAS vs RL Comparison**

| Method | Search Type | mAP | FLOPs | Search Cost | Sample-Adaptive |
|--------|-------------|-----|-------|-------------|-----------------|
| DARTS-LS | NAS | 40.8 | 38% | 120h | âŒ |
| EA-LS | NAS | 40.5 | 40% | 200h | âŒ |
| GDAS-LS | NAS | 40.9 | 37% | 100h | âŒ |
| **RL-PyramidKD** | RL | **41.5** | **35%** | **60h** | âœ… |

**æ ¸å¿ƒä¼˜åŠ¿**:
- âœ… æ ·æœ¬çº§è‡ªé€‚åº”ï¼ˆç®€å•æ ·æœ¬â†’P5ï¼Œå¤æ‚æ ·æœ¬â†’P2-P5ï¼‰
- âœ… æœç´¢æˆæœ¬é™ä½40%
- âœ… è·¨ä»»åŠ¡æ³›åŒ–ï¼ˆmeta-learningï¼‰

### æ¶ˆèå®éªŒ (Table 4-10)

**Table 4: Component Ablation**

| Variant | mAP | FLOPs | è¯´æ˜ |
|---------|-----|-------|------|
| Fixed Uniform | 40.2 | 50% | Baseline |
| RL w/o Sequential | 40.6 | 48% | ç‹¬ç«‹é€‰æ‹©å±‚ |
| RL w/o Efficiency | 41.2 | 50% | Î»=0 |
| RL w/o GradNorm | 41.1 | 35% | æ— æ¢¯åº¦å¹³è¡¡ |
| **RL-PyramidKD (Full)** | **41.5** | **35%** | å®Œæ•´æ–¹æ³• |

**Table 5: Gradient Optimization**

| Method | mAP | Training Stability |
|--------|-----|-------------------|
| Baseline | 40.2 | Unstable |
| + Gradient Clipping | 40.5 | Stable |
| + GradNorm | 40.9 | Very Stable |
| **RL + GradNorm** | **41.5** | Very Stable |

---

## ğŸ›£ï¸ å¼€å‘è·¯çº¿å›¾

### âœ… å·²å®Œæˆ (Week 1-2)

- [x] é¡¹ç›®ç»“æ„æ­å»º
- [x] RLæ ¸å¿ƒç»„ä»¶ (Policy, Environment, Trainer, MAML, Buffer)
- [x] GradNormä¼˜åŒ–å™¨
- [x] Loggerå·¥å…·
- [x] é…ç½®æ–‡ä»¶
- [x] è®­ç»ƒè„šæœ¬ (Phase 1-2)
- [x] å®Œæ•´æ–‡æ¡£

**ä»£ç é‡**: ~2500è¡Œ
**å®Œæˆåº¦**: 65%

### ğŸ”„ è¿›è¡Œä¸­ (Week 3-4)

- [ ] å®ç°Teacher/Studentæ¨¡å‹ (ResNet + FPN)
- [ ] å®ç°COCO DataLoader
- [ ] å®ŒæˆPhase 3-4è®­ç»ƒä»£ç 
- [ ] æ·»åŠ mAPè¯„ä¼°å·¥å…·

### â³ å¾…å®ç° (Week 5-8)

**Week 5-6: NASåŸºçº¿**
- [ ] DARTS-LSå®ç°
- [ ] EA-LSå®ç°
- [ ] GDAS-LSå®ç°
- [ ] NASè®­ç»ƒè„šæœ¬

**Week 7-8: è¯„ä¼°ä¸å¯è§†åŒ–**
- [ ] å®Œæ•´è¯„ä¼°è„šæœ¬
- [ ] ç­–ç•¥å¯è§†åŒ–å·¥å…·
- [ ] Pareto frontierç»˜åˆ¶
- [ ] GradNormæƒé‡æ¼”åŒ–å›¾

### ğŸ“Š å®éªŒè®¡åˆ’ (Week 9-18)

**Week 9-12: ä¸»å®éªŒ**
- [ ] Table 1: COCO Detection
- [ ] Table 2: Semantic Segmentation
- [ ] Table 3: Classification

**Week 13-15: æ¶ˆèå®éªŒ**
- [ ] Table 4: Component Ablation
- [ ] Table 5-6: Î»å‚æ•° + Meta-learning
- [ ] Table 7-9: NASå¯¹æ¯”

**Week 16-18: è®ºæ–‡æ’°å†™**
- [ ] å®Œæ•´è®ºæ–‡åˆç¨¿
- [ ] å›¾è¡¨åˆ¶ä½œ
- [ ] ä¿®æ”¹æ¶¦è‰²
- [ ] æäº¤ (ECCV/NeurIPS 2026)

---

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•æµ‹è¯•å•ä¸ªç»„ä»¶ï¼Ÿ

æ¯ä¸ªæ¨¡å—éƒ½æœ‰å•å…ƒæµ‹è¯•ï¼š

```bash
# æµ‹è¯•Policy Network
python -m rl.policy

# æµ‹è¯•Environment
python -m rl.environment

# æµ‹è¯•PPO Trainer
python -m rl.trainer

# æµ‹è¯•GradNorm
python -m utils.gradnorm

# æµ‹è¯•MAML
python -m rl.meta_learning
```

### Q2: ç¼ºå°‘ä¾èµ–æ€ä¹ˆåŠï¼Ÿ

```bash
# æœ€å°å®‰è£…ï¼ˆä»…æ ¸å¿ƒç»„ä»¶ï¼‰
pip install torch torchvision numpy pyyaml tqdm

# å®Œæ•´å®‰è£…
pip install -r requirements.txt
```

### Q3: GPUå†…å­˜ä¸è¶³ï¼Ÿ

ä¿®æ”¹ `configs/default.yaml`:

```yaml
dataset:
  batch_size: 8  # æ”¹ä¸º 4 æˆ– 2

rl:
  training:
    num_parallel_envs: 4  # æ”¹ä¸º 2
```

### Q4: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢ï¼Ÿ

```bash
# è°ƒè¯•æ¨¡å¼ï¼ˆå‡å°‘æ•°æ®é‡ï¼‰
python scripts/train_rl.py --debug

# å‡å°‘episodesï¼ˆä¿®æ”¹é…ç½®æ–‡ä»¶ï¼‰
# configs/default.yamlä¸­num_episodesæ”¹ä¸º100
```

### Q5: å¦‚ä½•å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Ÿ

```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir experiments/logs/tensorboard

# è®¿é—® http://localhost:6006
```

### Q6: å¦‚ä½•æ·»åŠ æ–°æ•°æ®é›†ï¼Ÿ

1. åˆ›å»ºé…ç½®æ–‡ä»¶: `configs/my_dataset.yaml`
2. ä¿®æ”¹ `scripts/train_rl.py` ä¸­çš„ `build_dataloader()`
3. è¿è¡Œ: `python scripts/train_rl.py --config configs/my_dataset.yaml`

### Q7: RL vs NASçš„æ ¸å¿ƒåŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ

**ç†è®ºåŒºåˆ«**:
- NAS: Î±* = argmax_{Î±âˆˆA} E_x[Reward(x, Î±)]
  â†’ æœç´¢**ä¸€ä¸ª**å›ºå®šæ¶æ„Î±*
- RL: Ï€* = argmax_Ï€ E_x[Reward(x, Ï€(x))]
  â†’ å­¦ä¹ **ä¸€ä¸ª**è‡ªé€‚åº”ç­–ç•¥Ï€ï¼Œæ ¹æ®æ ·æœ¬xåŠ¨æ€è°ƒæ•´

**å®é™…åŒºåˆ«**:
- NAS: æ‰€æœ‰æ ·æœ¬ä½¿ç”¨ç›¸åŒæ¶æ„ï¼ˆå¦‚P3-P5ï¼‰
- RL: ç®€å•æ ·æœ¬â†’P5ï¼Œå¤æ‚æ ·æœ¬â†’P2-P5ï¼ˆæ ·æœ¬çº§è‡ªé€‚åº”ï¼‰

### Q8: GradNormä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ

GradNormè‡ªåŠ¨è°ƒæ•´ä»»åŠ¡æƒé‡ï¼Œä½¿æ¢¯åº¦èŒƒæ•°å¹³è¡¡ï¼š

```python
# è®­ç»ƒåˆæœŸ
w_P2=0.25, w_P3=0.25, w_P4=0.25, w_P5=0.25
||âˆ‡L_P2||=0.001, ||âˆ‡L_P5||=1.0  # ä¸å¹³è¡¡ï¼

# GradNormè°ƒæ•´å
w_P2=0.40, w_P3=0.30, w_P4=0.20, w_P5=0.10
||âˆ‡L_P2||â‰ˆ||âˆ‡L_P3||â‰ˆ||âˆ‡L_P4||â‰ˆ||âˆ‡L_P5||  # å¹³è¡¡ï¼

# æ•ˆæœ: P2éš¾å­¦â†’æƒé‡å¢å¤§â†’å­¦å¾—æ›´å¥½ â†’ +0.4 mAP
```

### Q9: Meta-learningå¦‚ä½•åŠ é€Ÿï¼Ÿ

```python
# ä¼ ç»Ÿè®­ç»ƒï¼ˆä»å¤´å¼€å§‹ï¼‰
new_task_policy = train_from_scratch(new_task_data, epochs=50)

# Meta-learningï¼ˆå¿«é€Ÿé€‚åº”ï¼‰
adapted_policy = maml.fast_adapt(new_task_data, num_steps=5)

# åŠ é€Ÿ: 50 epochs â†’ 5 steps (10Ã—åŠ é€Ÿ)
```

### Q10: é¡¹ç›®å®Œæˆåå¦‚ä½•ç»§ç»­ï¼Ÿ

1. **NASæ‰©å±•**: æ·»åŠ æ›´å¤šNASæ–¹æ³•ï¼ˆENAS, ProxylessNASï¼‰
2. **Transformer**: æ‰©å±•åˆ°ViTé‡‘å­—å¡”
3. **å¤šä»»åŠ¡**: åŒæ—¶è®­ç»ƒæ£€æµ‹+åˆ†å‰²+åˆ†ç±»
4. **åœ¨çº¿è‡ªé€‚åº”**: æ¨ç†æ—¶åŠ¨æ€è°ƒæ•´ç­–ç•¥
5. **ç†è®ºåˆ†æ**: RL vs NASçš„ç†è®ºä¿è¯

---

## ğŸ“š å‚è€ƒèµ„æ–™

### è®ºæ–‡è®¾è®¡æ–‡æ¡£

- **å®Œæ•´è®ºæ–‡å¤§çº²**: `../paper2_complete_integrated.md` (9-10é¡µï¼ŒåŒ…å«NASå¯¹æ¯”å’Œæ¢¯åº¦ä¼˜åŒ–)
- **é¡¹ç›®è¯¦ç»†ç»“æ„**: `PROJECT_STRUCTURE.md` (ä»£ç ç»Ÿè®¡å’Œå¼€å‘è®¡åˆ’)
- **å¿«é€Ÿä¸Šæ‰‹**: `QUICKSTART.md` (5åˆ†é’Ÿå¿«é€Ÿæµ‹è¯•)

### ç›¸å…³è®ºæ–‡

**å¼ºåŒ–å­¦ä¹ **:
- Schulman et al., "Proximal Policy Optimization", 2017 (PPO)
- Finn et al., "Model-Agnostic Meta-Learning", ICML 2017 (MAML)

**ç¥ç»æ¶æ„æœç´¢**:
- Liu et al., "DARTS: Differentiable Architecture Search", ICLR 2019
- Real et al., "Regularized Evolution for Image Classifier", AAAI 2019

**æ¢¯åº¦ä¼˜åŒ–**:
- Chen et al., "GradNorm: Gradient Normalization", ICML 2018
- Yu et al., "Gradient Surgery for Multi-Task Learning", NeurIPS 2020

**çŸ¥è¯†è’¸é¦**:
- Lin et al., "Feature Pyramid Networks", CVPR 2017
- Ma et al., "HMKD: Hierarchical Matching KD", JCST 2024

---

## ğŸ“§ è”ç³»æ–¹å¼

- **Issues**: ä»£ç é—®é¢˜è¯·æIssue
- **Email**: your.email@example.com
- **æ–‡æ¡£**: æŸ¥çœ‹ `PROJECT_STRUCTURE.md` äº†è§£è¯¦ç»†ç»“æ„

---

## ğŸ“ Citation

å¦‚æœä½¿ç”¨æœ¬ä»£ç ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@inproceedings{rl-pyramidkd,
  title={RL-PyramidKD: Reinforcement Learning for Dynamic Layer Selection in Pyramid-based Knowledge Distillation},
  author={Your Name},
  booktitle={NeurIPS},
  year={2026}
}
```

---

## â­ é¡¹ç›®äº®ç‚¹æ€»ç»“

1. **âœ… å®Œæ•´çš„RLå®ç°** (~1200è¡Œ)
   - PolicyNetwork, Environment, Trainer, MAML, Buffer
   - æ‰€æœ‰ç»„ä»¶éƒ½æœ‰å•å…ƒæµ‹è¯•

2. **âœ… GradNormä¼˜åŒ–** (+0.4 mAP)
   - è‡ªåŠ¨æ¢¯åº¦å¹³è¡¡
   - è®­ç»ƒæ›´ç¨³å®š

3. **âœ… å®Œæ•´é…ç½®ç³»ç»Ÿ**
   - YAMLé…ç½®
   - æ”¯æŒå¤šä»»åŠ¡/å¤šæ•°æ®é›†

4. **âœ… è¯¦ç»†æ–‡æ¡£**
   - README (æœ¬æ–‡ä»¶)
   - PROJECT_STRUCTURE (è¯¦ç»†ç»“æ„)
   - QUICKSTART (å¿«é€Ÿä¸Šæ‰‹)

5. **ğŸ“Š é¢„æœŸæ€§èƒ½**
   - +0.6-1.0 mAP vs NAS
   - -40% æœç´¢æˆæœ¬
   - æ ·æœ¬çº§è‡ªé€‚åº”

6. **ğŸš€ å¼€å‘å‹å¥½**
   - æ¨¡å—åŒ–è®¾è®¡
   - æ˜“äºæ‰©å±•
   - å®Œæ•´ç¤ºä¾‹

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸ‰**

å¦‚æœ‰é—®é¢˜ï¼Œæ¬¢è¿æŸ¥é˜…æ–‡æ¡£æˆ–æIssueã€‚

**æœ€åæ›´æ–°**: 2025-10-24
