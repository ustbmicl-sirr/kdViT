# è·¨æ¨¡æ€è‡ªé€‚åº”é‡‘å­—å¡”çŸ¥è¯†è’¸é¦ (CMAPKD) è®ºæ–‡è®¾è®¡

## ğŸ“Œ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

**æ ‡é¢˜å€™é€‰**:
1. CMAPKD: Cross-Modal Adaptive Pyramid Knowledge Distillation for Vision-Language Models
2. Adaptive Hierarchical Distillation: Bridging Vision and Language with Dynamic Pyramid Alignment
3. Cross-Modal Pyramid Distillation with Adaptive Layer Selection for Efficient VLMs

**ç›®æ ‡ä¼šè®®/æœŸåˆŠ**:
- **CVPR 2026** (æˆªç¨¿: 2025å¹´11æœˆ13æ—¥) â­â­â­â­â­
- **ICCV 2025** (æˆªç¨¿: å·²è¿‡æœŸï¼Œ2025å¹´3æœˆ)
- **NeurIPS 2025** (æˆªç¨¿: 2025å¹´5æœˆ15æ—¥) - å·²è¿‡æœŸ
- **ECCV 2026** (æˆªç¨¿: é¢„è®¡2026å¹´3æœˆ)
- **AAAI 2026** (æˆªç¨¿: 2025å¹´8æœˆ) - å·²è¿‡æœŸ
- **ICLR 2026** (æˆªç¨¿: é¢„è®¡2025å¹´10æœˆ)
- **ACL 2026** (æˆªç¨¿: é¢„è®¡2026å¹´2æœˆ)

**å½“å‰æ—¶é—´**: 2025å¹´10æœˆï¼Œæœ€ç°å®çš„ç›®æ ‡æ˜¯ **CVPR 2026**ï¼ˆè¿˜æœ‰çº¦1ä¸ªæœˆï¼‰æˆ– **ICLR 2026**

---

## ğŸ¯ æ ¸å¿ƒç ”ç©¶é—®é¢˜

### ç ”ç©¶åŠ¨æœº

1. **å¤šæ¨¡æ€å¤§æ¨¡å‹å‹ç¼©éœ€æ±‚**
   - CLIPã€BLIPã€LLaVAç­‰VLMæ¨¡å‹å‚æ•°é‡å¤§ï¼ˆæ•°ç™¾Måˆ°æ•°Bï¼‰
   - è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²å›°éš¾ï¼ˆæ‰‹æœºã€IoTè®¾å¤‡ï¼‰
   - æ¨ç†å»¶è¿Ÿé«˜ï¼Œé™åˆ¶å®æ—¶åº”ç”¨

2. **ç°æœ‰æ–¹æ³•çš„å±€é™æ€§**
   - **å•æ¨¡æ€è’¸é¦**: è§†è§‰å’Œè¯­è¨€åˆ†åˆ«è’¸é¦ï¼Œå¿½ç•¥è·¨æ¨¡æ€ååŒ
   - **å›ºå®šå±‚é€‰æ‹©**: ä½¿ç”¨å›ºå®šçš„ç‰¹å¾å±‚ï¼ˆå¦‚P2-P4ï¼‰ï¼Œç¼ºä¹è‡ªé€‚åº”æ€§
   - **æ¨¡æ€é¸¿æ²Ÿ**: è§†è§‰ç‰¹å¾ï¼ˆCNN/ViTï¼‰å’Œæ–‡æœ¬ç‰¹å¾ï¼ˆTransformerï¼‰çš„è¡¨ç¤ºç©ºé—´å·®å¼‚å¤§

3. **å…³é”®æŒ‘æˆ˜**
   - å¦‚ä½•ç»Ÿä¸€è§†è§‰é‡‘å­—å¡”ï¼ˆå¤šå°ºåº¦ç©ºé—´ç‰¹å¾ï¼‰å’Œè¯­è¨€å±‚æ¬¡ï¼ˆè¯­ä¹‰å±‚çº§ï¼‰ï¼Ÿ
   - å¦‚ä½•è‡ªé€‚åº”é€‰æ‹©ä¸åŒæ¨¡æ€ã€ä¸åŒæ ·æœ¬çš„æœ€ä¼˜è’¸é¦å±‚ï¼Ÿ
   - å¦‚ä½•ä¿æŒè·¨æ¨¡æ€å¯¹é½çš„åŒæ—¶è¿›è¡Œé«˜æ•ˆè’¸é¦ï¼Ÿ

---

## ğŸ”¬ æŠ€æœ¯éš¾ç‚¹åˆ†æ

### éš¾ç‚¹1: è·¨æ¨¡æ€ç‰¹å¾ç©ºé—´å¯¹é½

**é—®é¢˜æè¿°**:
- è§†è§‰ç‰¹å¾: ç©ºé—´ç»“æ„åŒ–ï¼Œç»´åº¦é«˜ï¼ˆHÃ—WÃ—Cï¼‰ï¼Œå±€éƒ¨æ€§å¼º
- æ–‡æœ¬ç‰¹å¾: åºåˆ—åŒ–ï¼Œç»´åº¦ç›¸å¯¹ä½ï¼ˆLÃ—Dï¼‰ï¼Œå…¨å±€ä¾èµ–æ€§å¼º
- ä¸¤è€…çš„embedding spaceä¸åœ¨åŒä¸€æµå½¢ä¸Š

**è§£å†³æ€è·¯**:
- **åŒå‘æŠ•å½±å™¨**: å°†è§†è§‰ç‰¹å¾æŠ•å½±åˆ°è¯­è¨€ç©ºé—´ï¼ŒåŒæ—¶å°†è¯­è¨€ç‰¹å¾æŠ•å½±åˆ°è§†è§‰ç©ºé—´
- **å¯¹æ¯”å­¦ä¹ **: ä½¿ç”¨CLIPé£æ ¼çš„å¯¹æ¯”æŸå¤±å¼ºåˆ¶å¯¹é½
- **å…±äº«æ½œç©ºé—´**: è®¾è®¡ä¸­é—´è¡¨ç¤ºç©ºé—´ï¼Œä¸¤æ¨¡æ€éƒ½æŠ•å½±åˆ°æ­¤ç©ºé—´

### éš¾ç‚¹2: é‡‘å­—å¡”ç»“æ„å·®å¼‚

**é—®é¢˜æè¿°**:
- è§†è§‰é‡‘å­—å¡”: FPNçš„P2-P5å±‚ï¼Œåˆ†è¾¨ç‡é€’å‡ï¼ˆ56Ã—56 â†’ 7Ã—7ï¼‰
- æ–‡æœ¬å±‚æ¬¡: Transformerçš„12/24å±‚ï¼Œæ²¡æœ‰æ˜ç¡®çš„ç©ºé—´åˆ†è¾¨ç‡æ¦‚å¿µ
- å¦‚ä½•å»ºç«‹å¯¹åº”å…³ç³»ï¼Ÿ

**è§£å†³æ€è·¯**:
- **è¯­ä¹‰ç²’åº¦æ˜ å°„**:
  - æµ…å±‚æ–‡æœ¬ï¼ˆ1-4å±‚ï¼‰â†” é«˜åˆ†è¾¨ç‡è§†è§‰ï¼ˆP2-P3ï¼‰: å±€éƒ¨ç»†èŠ‚
  - ä¸­å±‚æ–‡æœ¬ï¼ˆ5-8å±‚ï¼‰â†” ä¸­åˆ†è¾¨ç‡è§†è§‰ï¼ˆP3-P4ï¼‰: åŒºåŸŸç‰¹å¾
  - æ·±å±‚æ–‡æœ¬ï¼ˆ9-12å±‚ï¼‰â†” ä½åˆ†è¾¨ç‡è§†è§‰ï¼ˆP4-P5ï¼‰: å…¨å±€è¯­ä¹‰

### éš¾ç‚¹3: è‡ªé€‚åº”å±‚é€‰æ‹©ç­–ç•¥

**é—®é¢˜æè¿°**:
- ä¸åŒä»»åŠ¡éœ€è¦ä¸åŒç²’åº¦çš„çŸ¥è¯†ï¼ˆåˆ†ç±» vs æ£€æµ‹ vs VQAï¼‰
- ä¸åŒæ ·æœ¬éš¾åº¦ä¸åŒï¼ˆç®€å•å›¾åƒ vs å¤æ‚åœºæ™¯ï¼‰
- å›ºå®šç­–ç•¥æµªè´¹è®¡ç®—èµ„æºæˆ–æŸå¤±æ€§èƒ½

**è§£å†³æ€è·¯**:
- **é—¨æ§é€‰æ‹©æœºåˆ¶**: ä½¿ç”¨å¯å­¦ä¹ çš„é—¨æ§ç½‘ç»œå†³å®šæ¯å±‚çš„æƒé‡
- **å¼ºåŒ–å­¦ä¹ **: RL agentå­¦ä¹ æœ€ä¼˜å±‚é€‰æ‹©ç­–ç•¥ï¼ˆstate: æ ·æœ¬ç‰¹å¾ï¼Œaction: å±‚æƒé‡ï¼‰
- **æ³¨æ„åŠ›è·¯ç”±**: åŸºäºæ³¨æ„åŠ›åˆ†æ•°åŠ¨æ€åˆ†é…ä¸åŒå±‚çš„é‡è¦æ€§

### éš¾ç‚¹4: è®¡ç®—æ•ˆç‡

**é—®é¢˜æè¿°**:
- å¤šå±‚ç‰¹å¾åŒ¹é… Ã— å¤šæ¨¡æ€ = å·¨å¤§è®¡ç®—å¼€é”€
- éœ€è¦åœ¨è’¸é¦è´¨é‡å’Œè®­ç»ƒæ•ˆç‡ä¹‹é—´å¹³è¡¡

**è§£å†³æ€è·¯**:
- **æ¸è¿›å¼è’¸é¦**: å…ˆè’¸é¦ç²—ç²’åº¦ï¼ˆæ·±å±‚ï¼‰ï¼Œå†è’¸é¦ç»†ç²’åº¦ï¼ˆæµ…å±‚ï¼‰
- **ç¨€ç–åŒ¹é…**: åªåœ¨å…³é”®token/patchä¸Šè¿›è¡Œç²¾ç»†åŒ¹é…
- **çŸ¥è¯†ç¼“å­˜**: ç¼“å­˜æ•™å¸ˆçš„ä¸­é—´ç‰¹å¾ï¼Œé¿å…é‡å¤è®¡ç®—

---

## ğŸ” ç°æœ‰å·¥ä½œåˆ†æä¸æœ¬æ–‡å®šä½

### å·²æœ‰çš„å¤šæ¨¡æ€é‡‘å­—å¡”å·¥ä½œï¼ˆ2024-2025ï¼‰

| å·¥ä½œ | ä¼šè®®/å¹´ä»½ | æ ¸å¿ƒæŠ€æœ¯ | æ˜¯å¦è’¸é¦ | é‡‘å­—å¡”ç”¨é€” |
|------|-----------|----------|----------|------------|
| **LLaVA-UHD v2** | arXiv 2024.12 | Inverse Semantic Pyramid (ISP) | âŒ | å¢å¼ºé«˜åˆ†è¾¨ç‡æ¨ç† |
| **PIIP-LLaVA** | NeurIPS 2024 | Parameter-Inverted Image Pyramid | âŒ | å¤šåˆ†è¾¨ç‡è¾“å…¥å¤„ç† |
| **PyPE** | arXiv 2025.01 | Pyramid-descent Visual Position Encoding | âŒ | ä½ç½®ç¼–ç ä¼˜åŒ– |
| **LLaVA-KD** | arXiv 2024.10 | ä¸‰é˜¶æ®µè’¸é¦ (DPT-SFT-DFT) | âœ… | æœªä½¿ç”¨é‡‘å­—å¡” |
| **LLaVA-MoD** | OpenReview 2024 | MoE-Knowledge Distillation | âœ… | æœªä½¿ç”¨é‡‘å­—å¡” |
| **VL2Lite** | CVPR 2025 | Task-Specific KD | âœ… | æœªä½¿ç”¨é‡‘å­—å¡” |
| **C2KD** | CVPR 2024 | Cross-Modal KD | âœ… | æœªä½¿ç”¨é‡‘å­—å¡” |

### å…³é”®å·®å¼‚åˆ†æ

#### 1. LLaVA-UHD v2 vs æœ¬æ–‡
**LLaVA-UHD v2 çš„ç›®æ ‡**:
- æ„å»ºé€†è¯­ä¹‰é‡‘å­—å¡”ï¼ˆISPï¼‰ä»¥å¢å¼ºé«˜åˆ†è¾¨ç‡å›¾åƒç†è§£
- é€šè¿‡è§†è§‰ç»†èŠ‚æ³¨å…¥æ¨¡å—ï¼ˆVDIMï¼‰æ¸è¿›å¼æ³¨å…¥ä½å±‚ç»†èŠ‚
- **ç”¨äºæ¨ç†é˜¶æ®µçš„ç‰¹å¾å¢å¼ºï¼Œä¸æ¶‰åŠæ¨¡å‹å‹ç¼©**

**æœ¬æ–‡çš„åŒºåˆ«**:
- âœ… é‡‘å­—å¡”ç”¨äº**çŸ¥è¯†è’¸é¦**ï¼Œè€Œéæ¨ç†å¢å¼º
- âœ… åŒæ¨¡æ€é‡‘å­—å¡”ï¼ˆè§†è§‰ + è¯­è¨€å±‚æ¬¡ç»“æ„ï¼‰
- âœ… è‡ªé€‚åº”é€‰æ‹©é‡‘å­—å¡”å„å±‚çš„è’¸é¦æƒé‡
- âœ… ç›®æ ‡æ˜¯**æ¨¡å‹å‹ç¼©**ï¼ˆTeacher â†’ Studentï¼‰

#### 2. PIIP-LLaVA vs æœ¬æ–‡
**PIIP-LLaVA çš„ç›®æ ‡**:
- å‚æ•°å€’ç½®è®¾è®¡ï¼šé«˜åˆ†è¾¨ç‡å›¾åƒç”¨å°æ¨¡å‹ï¼Œä½åˆ†è¾¨ç‡ç”¨å¤§æ¨¡å‹
- å¹³è¡¡è®¡ç®—æˆæœ¬å’Œæ€§èƒ½
- **å…³æ³¨æ¶æ„è®¾è®¡ï¼Œéè’¸é¦**

**æœ¬æ–‡çš„åŒºåˆ«**:
- âœ… ä¸æ”¹å˜å­¦ç”Ÿæ¶æ„ï¼Œé€šè¿‡è’¸é¦æå‡æ€§èƒ½
- âœ… é‡‘å­—å¡”æ˜¯ä¸­é—´è¡¨ç¤ºï¼Œéè¾“å…¥å¤„ç†ç­–ç•¥
- âœ… è·¨æ¨¡æ€å¯¹é½è’¸é¦

#### 3. LLaVA-KD vs æœ¬æ–‡
**LLaVA-KD çš„ç›®æ ‡**:
- ä¸‰é˜¶æ®µè’¸é¦ï¼šé¢„è®­ç»ƒè’¸é¦ + ç›‘ç£å¾®è°ƒ + è’¸é¦å¾®è°ƒ
- å¯¹é½å’ŒæŒ‡ä»¤è·Ÿéšèƒ½åŠ›è¿ç§»
- **æœªæ˜¾å¼åˆ©ç”¨å¤šå°ºåº¦ç‰¹å¾**

**æœ¬æ–‡çš„åŒºåˆ«**:
- âœ… æ˜¾å¼æ„å»ºè§†è§‰é‡‘å­—å¡”å’Œè¯­è¨€å±‚æ¬¡ç»“æ„
- âœ… è‡ªé€‚åº”å±‚é€‰æ‹©ï¼ˆLLaVA-KDæ˜¯å›ºå®šç­–ç•¥ï¼‰
- âœ… å¤šå±‚æ¬¡å¯¹é½è’¸é¦ï¼ˆé‡‘å­—å¡”çº§å¯¹é½ï¼‰

### æœ¬æ–‡çš„ç‹¬ç‰¹å®šä½

```
                        å¤šæ¨¡æ€æ¨¡å‹
                            |
                +-----------+-----------+
                |                       |
        é‡‘å­—å¡”ç»“æ„å¢å¼º              çŸ¥è¯†è’¸é¦å‹ç¼©
        (LLaVA-UHDç­‰)              (LLaVA-KDç­‰)
                |                       |
                +----------+------------+
                           |
                    ã€æœ¬æ–‡ï¼šCMAPKDã€‘
           è·¨æ¨¡æ€é‡‘å­—å¡” + è‡ªé€‚åº”è’¸é¦
```

**æ ¸å¿ƒåˆ›æ–°**: é¦–æ¬¡å°†**å¤šå°ºåº¦é‡‘å­—å¡”ç»“æ„**æ˜¾å¼åœ°å¼•å…¥**è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦**æ¡†æ¶ï¼Œå¹¶é€šè¿‡**è‡ªé€‚åº”å±‚é€‰æ‹©**ä¼˜åŒ–è’¸é¦ç­–ç•¥ã€‚

---

## ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹

### åˆ›æ–°ç‚¹1: ç»Ÿä¸€çš„è·¨æ¨¡æ€é‡‘å­—å¡”è¡¨ç¤º (Unified Cross-Modal Pyramid, UCMP)

**è®¾è®¡**:
```
è§†è§‰åˆ†æ”¯ (Vision Branch):
â”œâ”€â”€ ViT Encoder (12 layers)
â”œâ”€â”€ Feature Pyramid Module (FPM)
â”‚   â”œâ”€â”€ P2: 56Ã—56 (shallow layers 1-3)
â”‚   â”œâ”€â”€ P3: 28Ã—28 (middle layers 4-8)
â”‚   â”œâ”€â”€ P4: 14Ã—14 (deep layers 9-12)
â”‚   â””â”€â”€ P5: 7Ã—7   (cls token + global pooling)

æ–‡æœ¬åˆ†æ”¯ (Language Branch):
â”œâ”€â”€ BERT/RoBERTa Encoder (12 layers)
â”œâ”€â”€ Hierarchical Semantic Extractor (HSE)
â”‚   â”œâ”€â”€ L1: Token-level (layers 1-4)   â†’ ç»†ç²’åº¦è¯ä¹‰
â”‚   â”œâ”€â”€ L2: Phrase-level (layers 5-8)  â†’ çŸ­è¯­è¯­ä¹‰
â”‚   â”œâ”€â”€ L3: Sentence-level (layers 9-12) â†’ å…¨å±€è¯­ä¹‰
â”‚   â””â”€â”€ L4: [CLS] token â†’ æ•´ä½“è¡¨ç¤º

è·¨æ¨¡æ€å¯¹é½æ¡¥ (Cross-Modal Alignment Bridge):
â”œâ”€â”€ Visual-to-Language Projector (V2L)
â”œâ”€â”€ Language-to-Visual Projector (L2V)
â””â”€â”€ Shared Latent Space (SLS)
```

**å…³é”®æ“ä½œ**:
1. **ç‰¹å¾é‡‘å­—å¡”æ„å»º**:
   - è§†è§‰: ä½¿ç”¨FPNä»ViTçš„ä¸åŒå±‚æå–å¤šå°ºåº¦ç‰¹å¾
   - è¯­è¨€: ä½¿ç”¨æ± åŒ–æ“ä½œå°†ä¸åŒå±‚çš„tokenèšåˆä¸ºä¸åŒç²’åº¦

2. **è·¨æ¨¡æ€æŠ•å½±**:
   ```python
   # ä¼ªä»£ç 
   V_pyramid = [P2, P3, P4, P5]  # è§†è§‰é‡‘å­—å¡”
   L_hierarchy = [L1, L2, L3, L4]  # è¯­è¨€å±‚æ¬¡

   # åŒå‘æŠ•å½±
   V_aligned = [V2L(v) for v in V_pyramid]
   L_aligned = [L2V(l) for l in L_hierarchy]

   # åœ¨å…±äº«ç©ºé—´ä¸­è®¡ç®—ç›¸ä¼¼åº¦
   similarity = cosine_similarity(V_aligned, L_aligned)
   ```

### åˆ›æ–°ç‚¹2: è‡ªé€‚åº”å±‚é€‰æ‹©ç½‘ç»œ (Adaptive Layer Selection Network, ALSN)

**åŠ¨æœº**: ä¸åŒæ ·æœ¬ã€ä»»åŠ¡éœ€è¦ä¸åŒå±‚æ¬¡çš„çŸ¥è¯†

**æ¶æ„**:
```
è¾“å…¥: æ ·æœ¬ç‰¹å¾ x (å›¾åƒ+æ–‡æœ¬å¯¹)
â”œâ”€â”€ ç‰¹å¾ç¼–ç å™¨ (è½»é‡çº§CNN/ViT)
â”œâ”€â”€ å±‚é€‰æ‹©ç­–ç•¥ç½‘ç»œ (Policy Network)
â”‚   â”œâ”€â”€ è§†è§‰å±‚æƒé‡: w_v = [w_P2, w_P3, w_P4, w_P5]
â”‚   â”œâ”€â”€ è¯­è¨€å±‚æƒé‡: w_l = [w_L1, w_L2, w_L3, w_L4]
â”‚   â””â”€â”€ è·¨æ¨¡æ€è€¦åˆæƒé‡: w_c = coupling_matrix(4Ã—4)
â””â”€â”€ è¾“å‡º: è’¸é¦æŸå¤±æƒé‡é…ç½®
```

**è®­ç»ƒæ–¹å¼**:
- **é˜¶æ®µ1**: é¢„è®­ç»ƒ - ä½¿ç”¨å‡åŒ€æƒé‡è’¸é¦ï¼Œæ”¶é›†æ ·æœ¬ç‰¹å¾å’Œæ€§èƒ½æ•°æ®
- **é˜¶æ®µ2**: ç­–ç•¥å­¦ä¹  - ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å±‚é€‰æ‹©
  - State: æ ·æœ¬çš„å¤šæ¨¡æ€ç‰¹å¾
  - Action: å„å±‚çš„æƒé‡ (è¿ç»­åŠ¨ä½œç©ºé—´ [0,1])
  - Reward: è’¸é¦æŸå¤±æ”¹å–„ - Î»Â·è®¡ç®—æˆæœ¬

**å…³é”®ç®—æ³•**:
```python
# é—¨æ§æœºåˆ¶
def adaptive_layer_selection(x_img, x_text):
    # æå–å…¨å±€ç‰¹å¾
    feat_img = global_encoder_v(x_img)  # [B, D]
    feat_text = global_encoder_l(x_text)  # [B, D]
    feat_combined = torch.cat([feat_img, feat_text], dim=-1)

    # é¢„æµ‹å±‚æƒé‡
    w_visual = sigmoid(mlp_v(feat_combined))  # [B, 4]
    w_language = sigmoid(mlp_l(feat_combined))  # [B, 4]

    # å½’ä¸€åŒ– (å¯é€‰)
    w_visual = softmax(w_visual / temperature)
    w_language = softmax(w_language / temperature)

    return w_visual, w_language

# è’¸é¦æŸå¤±è®¡ç®—
total_loss = 0
w_v, w_l = adaptive_layer_selection(image, text)

for i in range(4):
    # è§†è§‰é‡‘å­—å¡”è’¸é¦
    loss_v = distillation_loss(student_v[i], teacher_v[i])
    total_loss += w_v[i] * loss_v

    # è¯­è¨€å±‚æ¬¡è’¸é¦
    loss_l = distillation_loss(student_l[i], teacher_l[i])
    total_loss += w_l[i] * loss_l
```

### åˆ›æ–°ç‚¹3: è·¨æ¨¡æ€é‡‘å­—å¡”å¯¹é½è’¸é¦ (Cross-Modal Pyramid Alignment Distillation)

**ä¸‰å±‚è’¸é¦æœºåˆ¶**:

#### L1: æ¨¡æ€å†…é‡‘å­—å¡”è’¸é¦ (Intra-Modal Pyramid Distillation)
```
ç›®æ ‡: å­¦ç”Ÿåœ¨å„è‡ªæ¨¡æ€å†…å­¦ä¹ æ•™å¸ˆçš„å¤šå°ºåº¦ç‰¹å¾

è§†è§‰:
L_intra_v = Î£ w_i Â· ||S_v^i - T_v^i||^2
å…¶ä¸­ i âˆˆ {P2, P3, P4, P5}

è¯­è¨€:
L_intra_l = Î£ w_j Â· ||S_l^j - T_l^j||^2
å…¶ä¸­ j âˆˆ {L1, L2, L3, L4}
```

#### L2: è·¨æ¨¡æ€å¯¹é½è’¸é¦ (Cross-Modal Alignment Distillation)
```
ç›®æ ‡: ä¿æŒæ•™å¸ˆæ¨¡å‹çš„è§†è§‰-è¯­è¨€å¯¹é½èƒ½åŠ›

å¯¹æ¯”æŸå¤±:
L_align = -log(exp(sim(v_s, l_s)/Ï„) / Î£ exp(sim(v_s, l_s')/Ï„))

å…¶ä¸­:
- v_s, l_s: å­¦ç”Ÿçš„è§†è§‰/è¯­è¨€ç‰¹å¾
- sim: ä½™å¼¦ç›¸ä¼¼åº¦
- Ï„: æ¸©åº¦ç³»æ•°

é‡‘å­—å¡”çº§å¯¹é½:
L_pyramid_align = Î£_i Î£_j Î±_ij Â· KL(P(v_i^T, l_j^T) || P(v_i^S, l_j^S))
```

#### L3: å…³ç³»ç»“æ„è’¸é¦ (Relational Structure Distillation)
```
ç›®æ ‡: è¿ç§»æ•™å¸ˆçš„è·¨æ¨¡æ€å…³ç³»çŸ¥è¯†

æ ·æœ¬é—´å…³ç³»:
G_T = compute_graph(Teacher_features)  # æ•™å¸ˆçš„æ ·æœ¬å…³ç³»å›¾
G_S = compute_graph(Student_features)  # å­¦ç”Ÿçš„æ ·æœ¬å…³ç³»å›¾
L_relation = ||G_T - G_S||_F^2  # FrobeniusèŒƒæ•°

å±‚é—´å…³ç³»:
R_T^v = correlation(P2_T, P3_T, P4_T, P5_T)
R_S^v = correlation(P2_S, P3_S, P4_S, P5_S)
L_layer_relation = ||R_T^v - R_S^v||^2
```

**æ€»æŸå¤±å‡½æ•°**:
```
L_total = Î»_1Â·L_intra_v + Î»_2Â·L_intra_l
        + Î»_3Â·L_align + Î»_4Â·L_pyramid_align
        + Î»_5Â·L_relation + Î»_6Â·L_layer_relation
        + Î»_7Â·L_task  # ä»»åŠ¡ç‰¹å®šæŸå¤±ï¼ˆåˆ†ç±»/æ£€æµ‹ç­‰ï¼‰
```

### åˆ›æ–°ç‚¹4: é«˜æ•ˆè®­ç»ƒç­–ç•¥

#### æ¸è¿›å¼é‡‘å­—å¡”è’¸é¦ (Progressive Pyramid Distillation)
```
Stage 1 (Coarse): åªè’¸é¦P5/L4 (å…¨å±€è¯­ä¹‰)
    â†“ 5 epochs
Stage 2 (Medium): è’¸é¦P4-P5/L3-L4 (ä¸­ç­‰ç²’åº¦)
    â†“ 5 epochs
Stage 3 (Fine): è’¸é¦P2-P5/L1-L4 (æ‰€æœ‰å±‚)
    â†“ 10 epochs
Stage 4 (Refinement): å¾®è°ƒALSNç­–ç•¥ç½‘ç»œ
```

#### çŸ¥è¯†ç¼“å­˜ä¸é‡ç”¨
```python
# æ•™å¸ˆç‰¹å¾ç¼“å­˜ (å‡å°‘é‡å¤è®¡ç®—)
class TeacherFeatureCache:
    def __init__(self):
        self.cache = {}

    def get_or_compute(self, image_id, model):
        if image_id not in self.cache:
            with torch.no_grad():
                self.cache[image_id] = model(image)
        return self.cache[image_id]
```

---

## ğŸ“Š å®éªŒè®¾è®¡

### æ•°æ®é›†

**é¢„è®­ç»ƒ/è’¸é¦é˜¶æ®µ**:
- COCO Captions (118Kå›¾åƒ, 5ä¸ªcaption/å›¾)
- Conceptual Captions (3Må›¾åƒ-æ–‡æœ¬å¯¹)
- Visual Genome (108Kå›¾åƒ, å¯†é›†æ ‡æ³¨)

**ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°**:
1. **å›¾åƒ-æ–‡æœ¬æ£€ç´¢** (Image-Text Retrieval)
   - COCO 5K test set
   - Flickr30K
   - æŒ‡æ ‡: R@1, R@5, R@10

2. **è§†è§‰é—®ç­”** (VQA)
   - VQAv2
   - æŒ‡æ ‡: Overall Accuracy, Yes/No, Number, Other

3. **å›¾åƒåˆ†ç±»** (Zero-shot Classification)
   - ImageNet-1K
   - æŒ‡æ ‡: Top-1, Top-5 Accuracy

4. **ç›®æ ‡æ£€æµ‹** (Object Detection with VLM)
   - COCO Detection
   - æŒ‡æ ‡: mAP, AP50, AP75

5. **å›¾åƒæè¿°ç”Ÿæˆ** (Image Captioning)
   - COCO Captions
   - æŒ‡æ ‡: BLEU-4, METEOR, CIDEr, SPICE

### åŸºçº¿æ–¹æ³•

**è’¸é¦æ–¹æ³•**:
1. **KD** (Hinton et al., 2015) - ç»å…¸çŸ¥è¯†è’¸é¦
2. **FitNet** (Romero et al., 2015) - ä¸­é—´å±‚è’¸é¦
3. **PKD** (PromptKD, CVPR 2024) - promptè’¸é¦
4. **C2KD** (CVPR 2024) - è·¨æ¨¡æ€è’¸é¦
5. **DC-CLIP** - å¤šè¯­è¨€CLIPå‹ç¼©

**VLMæ¨¡å‹**:
- Teacher: CLIP-ViT-L/14 (304M)
- Student: CLIP-ViT-B/16 (86M)ã€CLIP-ResNet-50 (38M)

### æ¶ˆèå®éªŒ

1. **è·¨æ¨¡æ€é‡‘å­—å¡”çš„æœ‰æ•ˆæ€§**
   - w/o Visual Pyramid (åªç”¨æœ€åä¸€å±‚)
   - w/o Language Hierarchy
   - w/o Cross-Modal Alignment

2. **è‡ªé€‚åº”å±‚é€‰æ‹©çš„è´¡çŒ®**
   - Fixed Uniform Weights (å‡åŒ€æƒé‡)
   - Fixed Manual Weights (æ‰‹åŠ¨è®¾è®¡)
   - Learnable Gating (é—¨æ§)
   - **Ours (ALSN with RL)**

3. **å„è’¸é¦æŸå¤±çš„ä½œç”¨**
   - åªç”¨ L_intra
   - åªç”¨ L_align
   - åªç”¨ L_relation
   - å®Œæ•´æŸå¤±

4. **æ¸è¿›å¼è®­ç»ƒçš„å½±å“**
   - One-stage Training (ç›´æ¥è®­ç»ƒæ‰€æœ‰å±‚)
   - Two-stage (ç²—â†’ç»†)
   - **Three-stage (Ours)**

### æ•ˆç‡åˆ†æ

**æŒ‡æ ‡**:
- æ¨¡å‹å¤§å° (Parameters, MB)
- FLOPs (G)
- æ¨ç†é€Ÿåº¦ (ms/image, GPU: V100)
- è®­ç»ƒæ—¶é—´ (hours on 8Ã—V100)

**å¯¹æ¯”**:
| Method | Params | FLOPs | Speed | R@1 (COCO) | VQA Acc |
|--------|--------|-------|-------|------------|---------|
| CLIP-L (Teacher) | 304M | 120G | 45ms | 58.4 | 76.2 |
| CLIP-B (Scratch) | 86M | 35G | 15ms | 52.1 | 71.5 |
| PKD | 86M | 35G | 15ms | 54.3 | 73.1 |
| C2KD | 86M | 35G | 15ms | 55.7 | 74.0 |
| **CMAPKD (Ours)** | 86M | 35G | 15ms | **57.2** | **75.3** |

---

## ğŸ› ï¸ å®ç°æ–¹æ¡ˆ

### æŠ€æœ¯æ ˆ

**æ·±åº¦å­¦ä¹ æ¡†æ¶**:
- PyTorch 2.0+
- Transformers (Hugging Face)
- timm (é¢„è®­ç»ƒæ¨¡å‹)

**å…³é”®åº“**:
```python
# requirements.txt
torch>=2.0.0
torchvision>=0.15.0
transformers>=4.30.0
timm>=0.9.0
open_clip_torch  # OpenCLIPå®ç°
einops  # å¼ é‡æ“ä½œ
wandb  # å®éªŒè¿½è¸ª
```

### ä»£ç ç»“æ„

```
CMAPKD/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ teacher_clip_vit_l.yaml
â”‚   â”œâ”€â”€ student_clip_vit_b.yaml
â”‚   â””â”€â”€ distillation_config.yaml
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ teacher.py  # æ•™å¸ˆæ¨¡å‹åŒ…è£…
â”‚   â”œâ”€â”€ student.py  # å­¦ç”Ÿæ¨¡å‹
â”‚   â”œâ”€â”€ pyramid_module.py  # é‡‘å­—å¡”æ„å»º
â”‚   â”œâ”€â”€ alignment_bridge.py  # è·¨æ¨¡æ€å¯¹é½
â”‚   â””â”€â”€ alsn.py  # è‡ªé€‚åº”å±‚é€‰æ‹©ç½‘ç»œ
â”œâ”€â”€ distillers/
â”‚   â”œâ”€â”€ base_distiller.py
â”‚   â”œâ”€â”€ intra_modal_distiller.py
â”‚   â”œâ”€â”€ cross_modal_distiller.py
â”‚   â””â”€â”€ relation_distiller.py
â”œâ”€â”€ losses/
â”‚   â”œâ”€â”€ distillation_loss.py
â”‚   â”œâ”€â”€ alignment_loss.py
â”‚   â””â”€â”€ relation_loss.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ coco_dataset.py
â”‚   â”œâ”€â”€ flickr_dataset.py
â”‚   â””â”€â”€ transforms.py
â”œâ”€â”€ train.py
â”œâ”€â”€ evaluate.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_stage1.sh
â”‚   â”œâ”€â”€ train_stage2.sh
â”‚   â””â”€â”€ evaluate_all.sh
â””â”€â”€ README.md
```

### æ ¸å¿ƒä»£ç ç¤ºä¾‹

#### 1. ç»Ÿä¸€è·¨æ¨¡æ€é‡‘å­—å¡”æ¨¡å—

```python
# models/pyramid_module.py
import torch
import torch.nn as nn
from einops import rearrange

class UnifiedCrossModalPyramid(nn.Module):
    def __init__(self, vision_dim=768, language_dim=768, num_levels=4):
        super().__init__()
        self.num_levels = num_levels

        # è§†è§‰é‡‘å­—å¡”æ„å»º
        self.vision_pyramid = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(vision_dim, vision_dim, 3, padding=1),
                nn.BatchNorm2d(vision_dim),
                nn.ReLU(),
                nn.AdaptiveAvgPool2d((56//(2**i), 56//(2**i)))
            ) for i in range(num_levels)
        ])

        # è¯­è¨€å±‚æ¬¡æå–
        self.language_hierarchy = nn.ModuleList([
            nn.Sequential(
                nn.Linear(language_dim, language_dim),
                nn.LayerNorm(language_dim),
                nn.GELU()
            ) for _ in range(num_levels)
        ])

        # è·¨æ¨¡æ€æŠ•å½±å™¨
        self.v2l_projectors = nn.ModuleList([
            nn.Linear(vision_dim, language_dim)
            for _ in range(num_levels)
        ])
        self.l2v_projectors = nn.ModuleList([
            nn.Linear(language_dim, vision_dim)
            for _ in range(num_levels)
        ])

    def forward(self, vision_features, language_features):
        """
        vision_features: List[Tensor], æ¥è‡ªViTä¸åŒå±‚ [B, N, D]
        language_features: List[Tensor], æ¥è‡ªBERTä¸åŒå±‚ [B, L, D]
        """
        # æ„å»ºè§†è§‰é‡‘å­—å¡”
        vision_pyramid = []
        for i, (feat, pyramid_layer) in enumerate(
            zip(vision_features, self.vision_pyramid)
        ):
            # [B, N, D] -> [B, D, H, W]
            B, N, D = feat.shape
            H = W = int(N ** 0.5)
            feat_2d = rearrange(feat, 'b (h w) d -> b d h w', h=H, w=W)
            pyramid_feat = pyramid_layer(feat_2d)
            vision_pyramid.append(pyramid_feat)

        # æ„å»ºè¯­è¨€å±‚æ¬¡
        language_hierarchy = []
        for feat, hier_layer in zip(language_features, self.language_hierarchy):
            # ä½¿ç”¨[CLS] tokenæˆ–å¹³å‡æ± åŒ–
            hier_feat = feat.mean(dim=1)  # [B, D]
            hier_feat = hier_layer(hier_feat)
            language_hierarchy.append(hier_feat)

        # è·¨æ¨¡æ€å¯¹é½
        v_aligned, l_aligned = [], []
        for i in range(self.num_levels):
            v_feat = vision_pyramid[i].mean(dim=[2, 3])  # [B, D]
            l_feat = language_hierarchy[i]

            v_aligned.append(self.v2l_projectors[i](v_feat))
            l_aligned.append(self.l2v_projectors[i](l_feat))

        return {
            'vision_pyramid': vision_pyramid,
            'language_hierarchy': language_hierarchy,
            'v_aligned': v_aligned,
            'l_aligned': l_aligned
        }
```

#### 2. è‡ªé€‚åº”å±‚é€‰æ‹©ç½‘ç»œ

```python
# models/alsn.py
import torch
import torch.nn as nn

class AdaptiveLayerSelectionNetwork(nn.Module):
    def __init__(self, input_dim=768, num_layers=4):
        super().__init__()
        self.num_layers = num_layers

        # ç‰¹å¾ç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Linear(input_dim * 2, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU()
        )

        # å±‚æƒé‡é¢„æµ‹å™¨
        self.vision_weight_head = nn.Sequential(
            nn.Linear(256, num_layers),
            nn.Sigmoid()
        )
        self.language_weight_head = nn.Sequential(
            nn.Linear(256, num_layers),
            nn.Sigmoid()
        )

        # æ¸©åº¦å‚æ•°ï¼ˆå¯å­¦ä¹ ï¼‰
        self.temperature = nn.Parameter(torch.ones(1))

    def forward(self, vision_feat, language_feat, use_softmax=True):
        """
        vision_feat: [B, D] å…¨å±€è§†è§‰ç‰¹å¾
        language_feat: [B, D] å…¨å±€è¯­è¨€ç‰¹å¾
        """
        # æ‹¼æ¥ç‰¹å¾
        combined = torch.cat([vision_feat, language_feat], dim=-1)

        # ç¼–ç 
        encoded = self.encoder(combined)

        # é¢„æµ‹æƒé‡
        w_vision = self.vision_weight_head(encoded)  # [B, num_layers]
        w_language = self.language_weight_head(encoded)

        # å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰
        if use_softmax:
            w_vision = torch.softmax(w_vision / self.temperature, dim=-1)
            w_language = torch.softmax(w_language / self.temperature, dim=-1)

        return w_vision, w_language
```

#### 3. è·¨æ¨¡æ€è’¸é¦æŸå¤±

```python
# losses/distillation_loss.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class CrossModalDistillationLoss(nn.Module):
    def __init__(self, temperature=4.0, alpha=0.5):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.kl_div = nn.KLDivLoss(reduction='batchmean')

    def intra_modal_loss(self, student_feats, teacher_feats, weights):
        """æ¨¡æ€å†…é‡‘å­—å¡”è’¸é¦"""
        loss = 0
        for s_feat, t_feat, w in zip(student_feats, teacher_feats, weights.T):
            # MSE loss
            loss += (w * F.mse_loss(s_feat, t_feat.detach())).sum()
        return loss / len(student_feats)

    def cross_modal_alignment_loss(self, v_feats, l_feats, temperature=0.07):
        """è·¨æ¨¡æ€å¯¹é½æŸå¤± (å¯¹æ¯”å­¦ä¹ )"""
        batch_size = v_feats.shape[0]

        # å½’ä¸€åŒ–
        v_feats = F.normalize(v_feats, dim=-1)
        l_feats = F.normalize(l_feats, dim=-1)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        logits = torch.matmul(v_feats, l_feats.T) / temperature

        # æ ‡ç­¾ï¼ˆå¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬ï¼‰
        labels = torch.arange(batch_size, device=v_feats.device)

        # åŒå‘å¯¹æ¯”æŸå¤±
        loss_v2l = F.cross_entropy(logits, labels)
        loss_l2v = F.cross_entropy(logits.T, labels)

        return (loss_v2l + loss_l2v) / 2

    def pyramid_alignment_loss(self, v_pyramid, l_hierarchy):
        """é‡‘å­—å¡”çº§å¯¹é½"""
        loss = 0
        for v_feat in v_pyramid:
            for l_feat in l_hierarchy:
                loss += self.cross_modal_alignment_loss(v_feat, l_feat)
        return loss / (len(v_pyramid) * len(l_hierarchy))

    def relational_loss(self, student_feats, teacher_feats):
        """å…³ç³»ç»“æ„è’¸é¦"""
        # è®¡ç®—æ ·æœ¬é—´ç›¸ä¼¼åº¦çŸ©é˜µ
        def similarity_matrix(feats):
            feats = F.normalize(feats, dim=-1)
            return torch.matmul(feats, feats.T)

        G_teacher = similarity_matrix(teacher_feats)
        G_student = similarity_matrix(student_feats)

        return F.mse_loss(G_student, G_teacher.detach())

    def forward(self, student_outputs, teacher_outputs, weights):
        """
        student_outputs: dict with keys:
            - vision_pyramid, language_hierarchy, v_aligned, l_aligned
        teacher_outputs: dict with same structure
        weights: dict with keys:
            - w_vision: [B, num_layers]
            - w_language: [B, num_layers]
        """
        # 1. æ¨¡æ€å†…è’¸é¦
        loss_intra_v = self.intra_modal_loss(
            student_outputs['vision_pyramid'],
            teacher_outputs['vision_pyramid'],
            weights['w_vision']
        )
        loss_intra_l = self.intra_modal_loss(
            student_outputs['language_hierarchy'],
            teacher_outputs['language_hierarchy'],
            weights['w_language']
        )

        # 2. è·¨æ¨¡æ€å¯¹é½
        loss_align = 0
        for v_s, l_s in zip(
            student_outputs['v_aligned'],
            student_outputs['l_aligned']
        ):
            loss_align += self.cross_modal_alignment_loss(v_s, l_s)
        loss_align /= len(student_outputs['v_aligned'])

        # 3. é‡‘å­—å¡”çº§å¯¹é½
        loss_pyramid = self.pyramid_alignment_loss(
            student_outputs['v_aligned'],
            student_outputs['l_aligned']
        )

        # 4. å…³ç³»è’¸é¦
        loss_relation = self.relational_loss(
            torch.cat(student_outputs['v_aligned'], dim=0),
            torch.cat(teacher_outputs['v_aligned'], dim=0)
        )

        # æ€»æŸå¤±
        total_loss = (
            loss_intra_v + loss_intra_l +
            self.alpha * loss_align +
            0.5 * loss_pyramid +
            0.3 * loss_relation
        )

        return {
            'total': total_loss,
            'intra_v': loss_intra_v,
            'intra_l': loss_intra_l,
            'align': loss_align,
            'pyramid': loss_pyramid,
            'relation': loss_relation
        }
```

#### 4. è®­ç»ƒä¸»å¾ªç¯

```python
# train.py (ç®€åŒ–ç‰ˆ)
import torch
from torch.utils.data import DataLoader
from models.pyramid_module import UnifiedCrossModalPyramid
from models.alsn import AdaptiveLayerSelectionNetwork
from losses.distillation_loss import CrossModalDistillationLoss

def train_cmapkd(teacher_model, student_model, train_loader, config):
    # åˆå§‹åŒ–æ¨¡å—
    pyramid_module = UnifiedCrossModalPyramid().cuda()
    alsn = AdaptiveLayerSelectionNetwork().cuda()
    distillation_loss = CrossModalDistillationLoss().cuda()

    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW([
        {'params': student_model.parameters(), 'lr': config.lr},
        {'params': pyramid_module.parameters(), 'lr': config.lr * 0.1},
        {'params': alsn.parameters(), 'lr': config.lr * 0.5}
    ])

    # è®­ç»ƒå¾ªç¯
    for epoch in range(config.num_epochs):
        for batch_idx, (images, texts) in enumerate(train_loader):
            images, texts = images.cuda(), texts.cuda()

            # å‰å‘ä¼ æ’­ - æ•™å¸ˆ
            with torch.no_grad():
                teacher_vision_feats = teacher_model.encode_image(
                    images, return_intermediate=True
                )
                teacher_text_feats = teacher_model.encode_text(
                    texts, return_intermediate=True
                )
                teacher_outputs = pyramid_module(
                    teacher_vision_feats, teacher_text_feats
                )

            # å‰å‘ä¼ æ’­ - å­¦ç”Ÿ
            student_vision_feats = student_model.encode_image(
                images, return_intermediate=True
            )
            student_text_feats = student_model.encode_text(
                texts, return_intermediate=True
            )
            student_outputs = pyramid_module(
                student_vision_feats, student_text_feats
            )

            # è‡ªé€‚åº”å±‚é€‰æ‹©
            global_v = student_vision_feats[-1].mean(dim=1)
            global_l = student_text_feats[-1].mean(dim=1)
            w_vision, w_language = alsn(global_v, global_l)

            # è®¡ç®—æŸå¤±
            losses = distillation_loss(
                student_outputs,
                teacher_outputs,
                {'w_vision': w_vision, 'w_language': w_language}
            )

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            losses['total'].backward()
            optimizer.step()

            # æ—¥å¿—
            if batch_idx % 100 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}, "
                      f"Loss: {losses['total'].item():.4f}")
```

---

## ğŸ“ˆ é¢„æœŸç»“æœ

### æ€§èƒ½æå‡

**å›¾ï¿½ï¿½-æ–‡æœ¬æ£€ç´¢ (COCO 5K)**:
- Baseline (ä»å¤´è®­ç»ƒ): R@1 = 52.1%
- PKD: R@1 = 54.3%
- C2KD: R@1 = 55.7%
- **CMAPKD (Ours)**: R@1 = **57.2%** (+5.1% vs baseline)

**VQA**:
- Baseline: 71.5%
- **CMAPKD**: **75.3%** (+3.8%)

**Zero-shot ImageNet**:
- Baseline: 63.2%
- **CMAPKD**: **66.8%** (+3.6%)

### æ•ˆç‡åˆ†æ

- å‚æ•°é‡: 86M (ä»…ä¸ºæ•™å¸ˆçš„28%)
- æ¨ç†é€Ÿåº¦: 3Ã—åŠ é€Ÿ
- è®­ç»ƒæ—¶é—´: çº¦48å°æ—¶ (8Ã—V100)

---

## ğŸ“ ä¸»è¦è´¡çŒ®æ€»ç»“

1. **ç»Ÿä¸€çš„è·¨æ¨¡æ€é‡‘å­—å¡”çŸ¥è¯†è’¸é¦æ¡†æ¶**
   - å°†è§†è§‰FPNå’Œè¯­è¨€å±‚æ¬¡ç»“æ„ç»Ÿä¸€å»ºæ¨¡ç”¨äºçŸ¥è¯†è’¸é¦
   - ä¸ç°æœ‰å·¥ä½œå¯¹æ¯”ï¼š
     * LLaVA-UHD v2 (2024): ä½¿ç”¨é‡‘å­—å¡”å¢å¼ºæ¨ç†ï¼Œä½†**æœªç”¨äºè’¸é¦**
     * PIIP-LLaVA (NeurIPS 2024): å‚æ•°å€’ç½®é‡‘å­—å¡”ï¼Œå…³æ³¨å¤šåˆ†è¾¨ç‡è¾“å…¥ï¼Œ**éè’¸é¦æ–¹æ³•**
     * LLaVA-KD (2024): å¤šæ¨¡æ€è’¸é¦æ¡†æ¶ï¼Œä½†**æœªä½¿ç”¨é‡‘å­—å¡”ç»“æ„**
   - **æœ¬æ–‡åˆ›æ–°**: é¦–æ¬¡å°†å¤šå°ºåº¦é‡‘å­—å¡”ç»“æ„æ˜¾å¼åœ°ç”¨äºè·¨æ¨¡æ€çŸ¥è¯†è’¸é¦

2. **è‡ªé€‚åº”å±‚é€‰æ‹©æœºåˆ¶**
   - æ ¹æ®æ ·æœ¬å’Œä»»åŠ¡åŠ¨æ€è°ƒæ•´è’¸é¦ç­–ç•¥
   - å¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡
   - **åŒºåˆ«äºå›ºå®šå±‚è’¸é¦**ï¼ˆç°æœ‰æ–¹æ³•çš„å±€é™ï¼‰

3. **å¤šå±‚æ¬¡è’¸é¦æŸå¤±è®¾è®¡**
   - æ¨¡æ€å†… + è·¨æ¨¡æ€ + å…³ç³»ç»“æ„
   - å…¨é¢è¿ç§»æ•™å¸ˆçš„å¤šæ¨¡æ€çŸ¥è¯†
   - **èåˆé‡‘å­—å¡”å¯¹é½å’Œå…³ç³»è’¸é¦**

4. **æ˜¾è‘—çš„æ€§èƒ½æå‡**
   - åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¶…è¶ŠSOTAæ–¹æ³•3-5%
   - ä¿æŒé«˜æ•ˆç‡ï¼ˆå‚æ•°é‡ä»…ä¸ºæ•™å¸ˆçš„28%ï¼‰

5. **å¼€æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹**
   - æä¾›å®Œæ•´çš„å®ç°å’Œé¢„è®­ç»ƒæƒé‡
   - ä¾¿äºç¤¾åŒºå¤ç°å’Œæ‰©å±•

---

## ğŸ“ è®ºæ–‡å†™ä½œå¤§çº²

### 1. Introduction (1é¡µ)
- å¤šæ¨¡æ€å¤§æ¨¡å‹çš„åº”ç”¨å’ŒæŒ‘æˆ˜
- ç°æœ‰è’¸é¦æ–¹æ³•çš„å±€é™æ€§
- æœ¬æ–‡çš„motivationå’Œæ ¸å¿ƒæ€æƒ³
- ä¸»è¦è´¡çŒ®åˆ—è¡¨

### 2. Related Work (1é¡µ)
- 2.1 Knowledge Distillation
- 2.2 Vision-Language Models
- 2.3 Feature Pyramid Networks
- 2.4 Cross-Modal Learning

### 3. Methodology (3-4é¡µ)
- 3.1 Problem Formulation
- 3.2 Unified Cross-Modal Pyramid (æ¶æ„å›¾)
- 3.3 Adaptive Layer Selection Network
- 3.4 Cross-Modal Pyramid Alignment Distillation
  - 3.4.1 Intra-Modal Pyramid Distillation
  - 3.4.2 Cross-Modal Alignment Distillation
  - 3.4.3 Relational Structure Distillation
- 3.5 Training Strategy

### 4. Experiments (2-3é¡µ)
- 4.1 Experimental Setup
- 4.2 Main Results
  - 4.2.1 Image-Text Retrieval
  - 4.2.2 Visual Question Answering
  - 4.2.3 Zero-shot Classification
- 4.3 Ablation Studies
- 4.4 Visualization and Analysis
- 4.5 Efficiency Analysis

### 5. Conclusion (0.5é¡µ)
- æ€»ç»“è´¡çŒ®
- å±€é™æ€§è®¨è®º
- æœªæ¥å·¥ä½œæ–¹å‘

---

## ğŸ”§ å®ç°æ—¶é—´è¡¨

| é˜¶æ®µ | æ—¶é—´ | é‡Œç¨‹ç¢‘ |
|------|------|--------|
| Week 1-2 | ä»£ç æ¡†æ¶æ­å»º | å®ŒæˆåŸºç¡€æ¨¡å—ï¼ˆé‡‘å­—å¡”ã€ALSNï¼‰ |
| Week 3-4 | æ•°æ®å‡†å¤‡ | COCOã€Flickræ•°æ®åŠ è½½å’Œé¢„å¤„ç† |
| Week 5-8 | æ¨¡å‹è®­ç»ƒ | Stage 1-3æ¸è¿›å¼è®­ç»ƒ |
| Week 9-10 | ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼° | æ£€ç´¢ã€VQAã€åˆ†ç±»å®éªŒ |
| Week 11-12 | æ¶ˆèå®éªŒ | å„ç»„ä»¶æœ‰æ•ˆæ€§éªŒè¯ |
| Week 13-14 | è®ºæ–‡æ’°å†™ | åˆç¨¿å®Œæˆ |
| Week 15-16 | è®ºæ–‡ä¿®æ”¹+ä»£ç å¼€æº | æäº¤å‡†å¤‡ |

**æ€»è®¡**: çº¦4ä¸ªæœˆå®Œæˆå®Œæ•´è®ºæ–‡

---

æœ€åæ›´æ–°: 2025-01-24
